{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Charger le fichier original\n",
        "df = pd.read_csv('paddydataset.csv', sep=',', encoding='utf-8', low_memory=False)\n",
        "\n",
        "# 2. Fonction pour appliquer le bruit\n",
        "def add_noise(df, uppercase_prob=0.10, missing_prob=0.05):\n",
        "    df_noisy = df.copy()\n",
        "\n",
        "    # Pour chaque colonne de type string (textuelle)\n",
        "    for col in df_noisy.select_dtypes(include=['object']).columns:\n",
        "        # Appliquer al√©atoirement la majuscule sur 10% des valeurs (par exemple)\n",
        "        mask = np.random.rand(len(df_noisy)) < uppercase_prob\n",
        "        df_noisy.loc[mask, col] = df_noisy.loc[mask, col].str.upper()\n",
        "\n",
        "    # Pour toutes les colonnes (num√©riques et textuelles) : supprimer al√©atoirement 5% des valeurs\n",
        "    for col in df_noisy.columns:\n",
        "        mask = np.random.rand(len(df_noisy)) < missing_prob\n",
        "        df_noisy.loc[mask, col] = np.nan  # ou '' si vous pr√©f√©rez une cha√Æne vide\n",
        "\n",
        "    return df_noisy\n",
        "\n",
        "# 3. Appliquer le bruit\n",
        "noisy_df = add_noise(df, uppercase_prob=0.10, missing_prob=0.08)\n",
        "\n",
        "# 4. Enregistrer dans un nouveau fichier\n",
        "noisy_df.to_csv('noisy_paddydataset.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Le dataset noisy a √©t√© enregistr√© avec succ√®s dans : noisy_paddydataset.csv\")\n",
        "print(f\"Nombre de lignes : {len(noisy_df)}\")\n",
        "print(f\"Pourcentage de valeurs manquantes total : {noisy_df.isna().mean().mean():.2%}\")"
      ],
      "metadata": {
        "id": "XZsITwiPkMUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNuLY9D_U4kQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import skew, kurtosis\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VdCpQH8U6-u"
      },
      "outputs": [],
      "source": [
        "# Configuration des graphiques\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP4xdI1DiQkS"
      },
      "source": [
        "---\n",
        "# 1. EDA (Exploratory Data Analysis)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOl5fd9zbppU"
      },
      "source": [
        "## 1.1 Chargement des donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsvnMzbDU8nH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Chemin relatif depuis le dossier racine du projet\n",
        "df = pd.read_csv(\"/content/noisy_paddydataset.csv\")\n",
        "\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"APER√áU G√âN√âRAL DU DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dimensions du dataset: {df.shape}\")\n",
        "print(f\"Nombre total de cellules: {df.shape[0] * df.shape[1]:,}\")\n",
        "print(df.head())\n",
        "print(\"=\"*60)\n",
        "print(\"APER√áU G√âN√âRAL DU DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dimensions du dataset: {df.shape}\")\n",
        "print(f\"Nombre total de cellules: {df.shape[0] * df.shape[1]:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93NMrEUVU-eL"
      },
      "outputs": [],
      "source": [
        "# Informations g√©n√©rales\n",
        "print(\"INFORMATIONS G√âN√âRALES:\")\n",
        "print(\"-\" * 30)\n",
        "df.info()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3MgigseVAYT"
      },
      "outputs": [],
      "source": [
        "# Statistiques descriptives de base\n",
        "print(\"STATISTIQUES DESCRIPTIVES - VARIABLES NUM√âRIQUES:\")\n",
        "print(\"-\" * 50)\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CBFW4yaidGF"
      },
      "source": [
        "## 1.2 Division des variables num√©riques et cat√©goriques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq5EaXt-Vfea"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 3. DISTINCTION VARIABLES NUM√âRIQUES ET CAT√âGORIQUES\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"DISTINCTION VARIABLES NUM√âRIQUES ET CAT√âGORIQUES\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Identification des types de variables\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Variables num√©riques: {len(numeric_features)}\")\n",
        "print(f\"  Exemples: {numeric_features[:10]}\")\n",
        "print(f\"\\nVariables cat√©goriques: {len(categorical_features)}\")\n",
        "print(f\"  Exemples: {categorical_features[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5vaI5eVirJV"
      },
      "source": [
        "## 1.3 V√©rification des valeurs manquantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XYTRVdQVCcx"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 2. ANALYSE DES DONN√âES MANQUANTES\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSE DES DONN√âES MANQUANTES\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvd4LhFxVD_G"
      },
      "outputs": [],
      "source": [
        "# Calcul des valeurs manquantes\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Colonnes': missing_data.index,\n",
        "    'Valeurs_Manquantes': missing_data.values,\n",
        "    'Pourcentage': missing_percent.values\n",
        "}).sort_values('Pourcentage', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGsylreUVFzL"
      },
      "outputs": [],
      "source": [
        "# Affichage des colonnes avec des valeurs manquantes\n",
        "missing_cols = missing_df[missing_df['Valeurs_Manquantes'] > 0]\n",
        "print(f\"Nombre de colonnes avec des valeurs manquantes: {len(missing_cols)}\")\n",
        "print(\"\\nColonnes avec le plus de valeurs manquantes:\")\n",
        "print(missing_cols.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation des donn√©es manquantes\n",
        "plt.figure(figsize=(15, 8))\n",
        "missing_cols_top = missing_cols.head(15)\n",
        "plt.barh(missing_cols_top['Colonnes'], missing_cols_top['Pourcentage'])\n",
        "plt.xlabel('Pourcentage de valeurs manquantes')\n",
        "plt.title('Top 15 des colonnes avec des valeurs manquantes')\n",
        "plt.gca().invert_yaxis()\n"
      ],
      "metadata": {
        "id": "0ulxYb6WnJ_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap des valeurs manquantes (√©chantillon)\n",
        "sample_cols = missing_cols.head(10)['Colonnes'].tolist()\n",
        "if sample_cols:\n",
        "    sns.heatmap(df[sample_cols].isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
        "    plt.title('Heatmap des valeurs manquantes\\n(Top 10 colonnes)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F1phmtWgnTvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data Cleaning / Data Preprocessing**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "E2xlFOhWo7i1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Visualisation : Histogrammes avec Mean & Median"
      ],
      "metadata": {
        "id": "EJqiATE7sS-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©er une grille d'histogrammes (max 3 par ligne)\n",
        "n_cols = 3\n",
        "n_rows = (len(numeric_features) + n_cols - 1) // n_cols\n",
        "\n",
        "plt.figure(figsize=(20, 5 * n_rows))\n",
        "\n",
        "for i, col in enumerate(numeric_features, 1):\n",
        "    plt.subplot(n_rows, n_cols, i)\n",
        "    data = df[col].dropna()\n",
        "\n",
        "    sns.histplot(data, kde=True, stat=\"density\", alpha=0.7, color='skyblue', linewidth=0)\n",
        "\n",
        "    mean_val = data.mean()\n",
        "    median_val = data.median()\n",
        "\n",
        "    plt.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Moyenne: {mean_val:.2f}')\n",
        "    plt.axvline(median_val, color='green', linestyle='-', linewidth=2, label=f'M√©diane: {median_val:.2f}')\n",
        "\n",
        "    skewness = stats.skew(data)\n",
        "    plt.title(f'{col}\\\\nSkewness: {skewness:.2f}', fontsize=12)\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Densit√©')\n",
        "    plt.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Distributions des Variables Num√©riques (avec Moyenne & M√©diane)', fontsize=16, y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g_rT8NFzpDvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Tests Statistiques & Suggestion d'Imputation"
      ],
      "metadata": {
        "id": "CPjRIelJsOqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse statistique pour chaque colonne num√©rique\n",
        "imputation_suggestions = []\n",
        "\n",
        "for col in numeric_features:\n",
        "    data = df[col].dropna()\n",
        "    n = len(data)\n",
        "\n",
        "    if n < 3:\n",
        "        continue  # Pas assez de donn√©es pour les tests\n",
        "\n",
        "    mean = data.mean()\n",
        "    median = data.median()\n",
        "    skewness = stats.skew(data)\n",
        "    kurt = stats.kurtosis(data)\n",
        "\n",
        "    # Test de normalit√© selon taille d'√©chantillon\n",
        "    if n <= 500:\n",
        "        # Shapiro-Wilk (p-value > 0.05 ‚Üí normal)\n",
        "        shapiro_stat, shapiro_p = stats.shapiro(data)\n",
        "        test_name = 'Shapiro-Wilk'\n",
        "        p_value = shapiro_p\n",
        "    elif n <= 5000:\n",
        "        # Kolmogorov-Smirnov (donn√©es standardis√©es)\n",
        "        standardized_data = (data - mean) / data.std()\n",
        "        ks_stat, ks_p = stats.kstest(standardized_data, 'norm')\n",
        "        test_name = 'Kolmogorov-Smirnov'\n",
        "        p_value = ks_p\n",
        "    else:\n",
        "        # Trop grand dataset ‚Üí on se fie √† skewness\n",
        "        p_value = np.nan\n",
        "        test_name = 'N/A'\n",
        "\n",
        "    # D√©cision d'imputation\n",
        "    if abs(skewness) <= 0.5 and (np.isnan(p_value) or p_value > 0.05):\n",
        "        imputation = \"Moyenne (distribution sym√©trique et normale)\"\n",
        "    else:\n",
        "        imputation = \"M√©diane (distribution asym√©trique ou non normale)\"\n",
        "\n",
        "    imputation_suggestions.append({\n",
        "        'Colonne': col,\n",
        "        'Moyenne': round(mean, 2),\n",
        "        'M√©diane': round(median, 2),\n",
        "        'Skewness': round(skewness, 3),\n",
        "        'Test Normalit√©': test_name,\n",
        "        'p-value': round(p_value, 4) if not np.isnan(p_value) else 'N/A',\n",
        "        'Suggestion Imputation': imputation\n",
        "    })\n",
        "\n",
        "# Affichage des r√©sultats\n",
        "results_df = pd.DataFrame(imputation_suggestions)\n",
        "results_df = results_df.sort_values(by='Skewness', key=abs, ascending=False)\n",
        "\n",
        "print(\"R√âSUM√â STATISTIQUE ET SUGGESTIONS D'IMPUTATION\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "HrbbQjmusXLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Application de l'Imputation Recommand√©e"
      ],
      "metadata": {
        "id": "snzPtRdBtDKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputation automatique selon les suggestions\n",
        "for col in numeric_features:\n",
        "    data = df[col]\n",
        "    if data.isnull().sum() == 0:\n",
        "        continue  # Rien √† imputer\n",
        "\n",
        "    # R√©cup√©rer la suggestion\n",
        "    suggestion = results_df.loc[results_df['Colonne'] == col, 'Suggestion Imputation'].values[0]\n",
        "\n",
        "    if \"Moyenne\" in suggestion:\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "        print(f\"{col} : imput√© par la MOYENNE\")\n",
        "    else:\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "        print(f\"{col} : imput√© par la M√âDIANE\")\n",
        "\n",
        "# V√©rification finale\n",
        "print(f\"\\nValeurs manquantes restantes (num√©riques) : {df[numeric_features].isnull().sum().sum()}\")"
      ],
      "metadata": {
        "id": "9ylssK-5tFoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 D√©tection des Outliers"
      ],
      "metadata": {
        "id": "YTj1EHc0ybFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apr√®s imputation des valeurs aberrantes\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "def auto_treat_outliers(df, numeric_features, min_rows_threshold=5000, plot=True):\n",
        "    \"\"\"\n",
        "    Traite automatiquement les outliers en choisissant la m√©thode la plus ad√©quate.\n",
        "\n",
        "    Crit√®res de d√©cision :\n",
        "    - Si tr√®s peu d'outliers (<1%) et dataset grand ‚Üí Suppression possible\n",
        "    - Si skewness > 1 et valeurs >=0 ‚Üí Transformation log\n",
        "    - Si % outliers > 5% ‚Üí Capping percentiles 5%/95%\n",
        "    - Sinon ‚Üí Robust IQR + remplacement par m√©diane\n",
        "    \"\"\"\n",
        "\n",
        "    summary = []\n",
        "    rows_to_drop = set()\n",
        "    initial_rows = len(df)\n",
        "\n",
        "    # üîπ Sauvegarde des donn√©es AVANT traitement (pour visualisation)\n",
        "    df_before = df.copy()\n",
        "\n",
        "    print(\"TRAITEMENT AUTOMATIQUE DES OUTLIERS - CHOIX INTELLIGENT PAR COLONNE\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    # ============================\n",
        "    # TRAITEMENT COLONNE PAR COLONNE\n",
        "    # ============================\n",
        "    for col in numeric_features:\n",
        "        data = df[col].dropna()\n",
        "\n",
        "        if len(data) < 10:\n",
        "            summary.append({\n",
        "                'Colonne': col,\n",
        "                'M√©thode Choisie': 'Ignor√© (trop peu de donn√©es)'\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Statistiques\n",
        "        skewness = stats.skew(data)\n",
        "        median = data.median()\n",
        "\n",
        "        # D√©tection IQR\n",
        "        Q1 = data.quantile(0.25)\n",
        "        Q3 = data.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - 1.5 * IQR\n",
        "        upper = Q3 + 1.5 * IQR\n",
        "\n",
        "        outlier_mask = (df[col] < lower) | (df[col] > upper)\n",
        "        nb_outliers = outlier_mask.sum()\n",
        "        percent_outliers = nb_outliers / len(df) * 100\n",
        "\n",
        "        method_chosen = \"\"\n",
        "        action = \"\"\n",
        "\n",
        "        # ============================\n",
        "        # D√âCISION AUTOMATIQUE\n",
        "        # ============================\n",
        "        if nb_outliers == 0:\n",
        "            method_chosen = \"Aucun outlier\"\n",
        "            action = \"Rien √† faire\"\n",
        "\n",
        "        elif percent_outliers < 1 and len(df) > min_rows_threshold:\n",
        "            rows_to_drop.update(df.loc[outlier_mask].index)\n",
        "            method_chosen = \"Suppression\"\n",
        "            action = f\"{nb_outliers} lignes supprim√©es\"\n",
        "\n",
        "        elif abs(skewness) > 1 and data.min() >= 0:\n",
        "            df[col + \"_log\"] = np.log1p(df[col])\n",
        "            method_chosen = \"Transformation log\"\n",
        "            action = \"Nouvelle colonne _log cr√©√©e\"\n",
        "\n",
        "        elif percent_outliers > 5:\n",
        "            df[col] = winsorize(df[col], limits=[0.05, 0.05])\n",
        "            method_chosen = \"Capping (5%/95%)\"\n",
        "            action = \"Winsorization appliqu√©e\"\n",
        "\n",
        "        else:\n",
        "            df.loc[outlier_mask, col] = median\n",
        "            method_chosen = \"Robust IQR + m√©diane\"\n",
        "            action = f\"{nb_outliers} outliers remplac√©s par m√©diane\"\n",
        "\n",
        "        summary.append({\n",
        "            'Colonne': col,\n",
        "            'Skewness': round(skewness, 3),\n",
        "            '% Outliers': round(percent_outliers, 2),\n",
        "            'M√©thode Choisie': method_chosen,\n",
        "            'Action': action\n",
        "        })\n",
        "\n",
        "    # ============================\n",
        "    # SUPPRESSION FINALE (UNE FOIS)\n",
        "    # ============================\n",
        "    if rows_to_drop:\n",
        "        df.drop(index=list(rows_to_drop), inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        df_before = df_before.loc[df.index].reset_index(drop=True)\n",
        "\n",
        "    # ============================\n",
        "    # VISUALISATION AVANT / APR√àS\n",
        "    # ============================\n",
        "    if plot:\n",
        "        fig, axes = plt.subplots(\n",
        "            nrows=len(numeric_features),\n",
        "            ncols=2,\n",
        "            figsize=(14, 4 * len(numeric_features))\n",
        "        )\n",
        "\n",
        "        if len(numeric_features) == 1:\n",
        "            axes = np.array([axes])\n",
        "\n",
        "        for i, col in enumerate(numeric_features):\n",
        "            sns.boxplot(\n",
        "                y=df_before[col],\n",
        "                ax=axes[i, 0],\n",
        "                color='lightcoral'\n",
        "            )\n",
        "            axes[i, 0].set_title(f'Avant - {col}')\n",
        "\n",
        "            sns.boxplot(\n",
        "                y=df[col],\n",
        "                ax=axes[i, 1],\n",
        "                color='lightgreen'\n",
        "            )\n",
        "            axes[i, 1].set_title(f'Apr√®s - {col}')\n",
        "\n",
        "        plt.suptitle(\"Comparaison AVANT / APR√àS par colonne\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # ============================\n",
        "    # R√âCAPITULATIF\n",
        "    # ============================\n",
        "    final_rows = len(df)\n",
        "    print(f\"\\nNombre de lignes : {initial_rows} ‚Üí {final_rows}\")\n",
        "\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    print(\"\\nR√âCAPITULATIF DU TRAITEMENT AUTOMATIQUE\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    return df, summary_df"
      ],
      "metadata": {
        "id": "egNCkWQpydkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_original = df.copy()\n",
        "df, recap = auto_treat_outliers(df, numeric_features, plot=True)\n",
        "\n",
        "\n",
        "print(\"\\n=== DATAFRAME APR√àS TRAITEMENT ===\")\n",
        "display(df)"
      ],
      "metadata": {
        "id": "0JDDWeY1L0Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== R√âCAPITULATIF ===\")\n",
        "display(recap)"
      ],
      "metadata": {
        "id": "0zTSkpW0NU-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.5 Imputation des Valeurs Manquantes & Uniformisation des Variables Cat√©gorielles"
      ],
      "metadata": {
        "id": "HOgR9ZIR7BK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 2.5 Imputation + Uniformisation des Variables Cat√©gorielles\n",
        "# ===================================================================\n",
        "\n",
        "print(\"√âTAT AVANT TRAITEMENT\")\n",
        "print(\"=\"*80)\n",
        "for col in categorical_features:\n",
        "    if col in df.columns:\n",
        "        missing = df[col].isnull().sum()\n",
        "        uniques = df[col].nunique()\n",
        "        print(f\"{col:30} ‚Üí {missing:3} valeurs manquantes | {uniques} valeurs uniques\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAITEMENT : IMPUTATION (mode) + UNIFORMISATION (minuscules + strip)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "treatment_summary = []\n",
        "\n",
        "for col in categorical_features:\n",
        "    if col not in df.columns:\n",
        "        print(f\"‚ö†Ô∏è {col} : colonne non pr√©sente\")\n",
        "        continue\n",
        "\n",
        "    initial_missing = df[col].isnull().sum()\n",
        "    initial_unique = df[col].nunique()\n",
        "\n",
        "    # 1. Imputation des valeurs manquantes par le mode\n",
        "    if initial_missing > 0:\n",
        "        mode_value = df[col].mode(dropna=True)[0]\n",
        "        df[col].fillna(mode_value, inplace=True)\n",
        "        imputed = True\n",
        "    else:\n",
        "        mode_value = None\n",
        "        imputed = False\n",
        "\n",
        "    # 2. Uniformisation : minuscules + suppression espaces\n",
        "    df[col] = df[col].astype(str).str.lower().str.strip()\n",
        "\n",
        "    # Apr√®s traitement\n",
        "    final_missing = df[col].isnull().sum()\n",
        "    final_unique = df[col].nunique()\n",
        "\n",
        "    print(f\"‚úì {col}\")\n",
        "    if imputed:\n",
        "        print(f\"   ‚Üí {initial_missing} valeurs manquantes imput√©es par mode : '{mode_value}'\")\n",
        "    print(f\"   ‚Üí Valeurs uniques : {initial_unique} ‚Üí {final_unique} (r√©duction des doublons de casse)\")\n",
        "\n",
        "    treatment_summary.append({\n",
        "        'Colonne': col,\n",
        "        'Manquantes Initiales': initial_missing,\n",
        "        'Imput√© par Mode': mode_value if imputed else 'Non',\n",
        "        'Uniques Avant': initial_unique,\n",
        "        'Uniques Apr√®s': final_unique\n",
        "    })\n",
        "\n",
        "# V√©rification finale\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"V√âRIFICATION FINALE\")\n",
        "print(\"=\"*80)\n",
        "total_missing_cat = df[categorical_features].isnull().sum().sum()\n",
        "print(f\"Valeurs manquantes restantes (cat√©gorielles) : {total_missing_cat} ‚Üí doit √™tre 0\")\n",
        "\n",
        "print(\"\\nValeurs uniques finales par colonne :\")\n",
        "for col in categorical_features:\n",
        "    if col in df.columns:\n",
        "        print(f\"   {col:30} ‚Üí {df[col].nunique()} valeurs : {sorted(df[col].unique())[:10]}...\")\n",
        "\n",
        "# Tableau r√©capitulatif\n",
        "summary_df = pd.DataFrame(treatment_summary)\n",
        "print(\"\\nR√âCAPITULATIF DU TRAITEMENT\")\n",
        "print(summary_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "pQDUznFT7Eyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset apr√®s preprocessing:\")\n",
        "print(\"=\"*80)\n",
        "# Affichage du dataset apr√®s preprocessing\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8o4HEz6uLYFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin du fichier de sortie (tu peux changer le nom si tu veux)\n",
        "cleaned_file_path = 'cleaned_paddydataset.csv'\n",
        "\n",
        "# Enregistrement du DataFrame nettoy√© (df est ton DataFrame apr√®s cleaning)\n",
        "df.to_csv(cleaned_file_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Dataset nettoy√© enregistr√© avec succ√®s !\")\n",
        "print(f\"Fichier : {cleaned_file_path}\")\n",
        "print(f\"Dimensions finales : {df.shape}\")\n",
        "print(f\"Aper√ßu des premi√®res lignes :\\n{df.head()}\")"
      ],
      "metadata": {
        "id": "Z9GjiPzIRbaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Encodage des Variables Cat√©gorielles"
      ],
      "metadata": {
        "id": "loxNk2v_jtU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 3. Encodage des Variables Cat√©gorielles\n",
        "# ========================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"ENCODAGE DES VARIABLES CAT√âGORIELLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# V√©rification du nombre de cat√©gories uniques (avant encodage)\n",
        "print(\"Nombre de cat√©gories par variable :\")\n",
        "for col in categorical_features:\n",
        "    if col in df.columns:\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"   {col:30} ‚Üí {unique_count} cat√©gories uniques\")\n",
        "    else:\n",
        "        print(f\"   {col:30} ‚Üí COLONNE MANQUANTE !\")\n",
        "\n",
        "# Application de One-Hot Encoding\n",
        "# drop_first=True : √©vite la multicolin√©arit√© en supprimant une cat√©gorie redondante par variable\n",
        "# dtype=int : pour avoir 0/1 au lieu de bool\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True, dtype=int)\n",
        "\n",
        "print(f\"\\nDimensions avant encodage : {df.shape}\")\n",
        "print(f\"Dimensions apr√®s encodage  : {df_encoded.shape}\")\n",
        "\n",
        "# Calcul du nombre de nouvelles colonnes cr√©√©es\n",
        "new_columns_created = df_encoded.shape[1] - (df.shape[1] - len(categorical_features))\n",
        "print(f\"Nouvelles colonnes binaires cr√©√©es : {new_columns_created}\")\n",
        "\n",
        "# Exemple des nouvelles colonnes g√©n√©r√©es\n",
        "print(\"\\nExemples de colonnes One-Hot cr√©√©es :\")\n",
        "new_cols = [col for col in df_encoded.columns if col not in df.columns]\n",
        "print(new_cols[:20])  # Affiche les 20 premi√®res\n",
        "\n",
        "# Mise √† jour du DataFrame pour la suite du notebook\n",
        "df = df_encoded.copy()\n",
        "\n",
        "# Aper√ßu final\n",
        "print(\"\\nAper√ßu du dataset encod√© :\")\n",
        "print(df.head())\n",
        "print(f\"\\nTypes de colonnes : {df.dtypes.value_counts()}\")"
      ],
      "metadata": {
        "id": "k8TS_-iKjqm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis (EDA) approfondie**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ByMLgLcZZLpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "pRY0AkC1XkcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse rapide de la variable cible pendant l'EDA\n",
        "target = 'Paddy yield(in Kg)'\n",
        "\"\"\"\n",
        "Rendement du riz en kilogrammes par parcelle.\n",
        "C‚Äôest une variable continue ‚Üí donc probl√®me principal est un probl√®me de r√©gression.\n",
        "\"\"\"\n",
        "print(f\"Variable cible : {target}\")\n",
        "print(f\"Type : {df[target].dtype}\")\n",
        "print(f\"Valeurs manquantes : {df[target].isnull().sum()}\")\n",
        "print(\"\\nStatistiques descriptives :\")\n",
        "print(df[target].describe())\n",
        "\n",
        "# Distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df[target], kde=True, color='skyblue')\n",
        "plt.title(f'Distribution de {target}')\n",
        "plt.xlabel(target)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df[target], color='lightgreen')\n",
        "plt.title(f'Boxplot de {target}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_QXzH5kDZfjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V√©rifier que la cible est bien pr√©sente\n",
        "target = 'Paddy yield(in Kg)'\n",
        "if target not in numeric_features:\n",
        "    print(f\"Attention : la cible '{target}' n'est pas dans les colonnes num√©riques !\")\n",
        "else:\n",
        "    print(f\"Calcul de la matrice de corr√©lation sur {len(numeric_features)} variables num√©riques\")\n",
        "\n",
        "# ========================================\n",
        "# 1. Matrice de corr√©lation compl√®te\n",
        "# ========================================\n",
        "corr_matrix = df[numeric_features].corr()\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(corr_matrix,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            linewidths=0.5,\n",
        "            cbar_kws={\"shrink\": 0.8},\n",
        "            mask=np.abs(corr_matrix) < 0.01)  # Masque les tr√®s faibles corr√©lations pour lisibilit√©\n",
        "plt.title('Matrice de Corr√©lation des Variables Num√©riques',\n",
        "          fontsize=18, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 2. Corr√©lation avec la variable cible\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CORR√âLATION AVEC LA VARIABLE CIBLE : Paddy yield(in Kg)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Corr√©lations avec la cible, tri√©es par valeur absolue (les plus fortes en haut)\n",
        "target_corr = corr_matrix[target].drop(target)  # On enl√®ve la corr√©lation de la cible avec elle-m√™me (1.0)\n",
        "target_corr_abs = target_corr.abs().sort_values(ascending=False)\n",
        "\n",
        "print(\"Top 20 variables les plus corr√©l√©es (valeur absolue) :\")\n",
        "print(\"-\"*80)\n",
        "print(target_corr_abs.head(20).round(3))\n",
        "\n",
        "print(\"\\nToutes les corr√©lations (tri√©es par force) :\")\n",
        "print(\"-\"*80)\n",
        "print(pd.DataFrame({\n",
        "    'Variable': target_corr_abs.index,\n",
        "    'Corr√©lation': target_corr[target_corr_abs.index].round(3)\n",
        "}).to_string(index=False))\n",
        "\n",
        "# Option : Barplot des top corr√©lations\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_n = 15\n",
        "sns.barplot(x=target_corr_abs.head(top_n).values,\n",
        "            y=target_corr_abs.head(top_n).index,\n",
        "            palette='viridis')\n",
        "plt.title(f'Top {top_n} Variables les plus Corr√©l√©es avec Paddy Yield (valeur absolue)',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Corr√©lation Absolue')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HNuL9mpVapEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Interpr√©tation :**\n",
        "\n",
        "- Le rendement est tr√®s sensible aux apports de nutriments (Micronutrients, Potassium, Urea, DAP) et aux tailles de parcelles / surfaces de semis (Hectares, Nursery area, LP_nurseryarea, LP_Mainfield).\n",
        "\n",
        "- L‚Äôutilisation de pesticides (Pest_60Day) et de d√©sherbants (Weed28D_thiobencarb) est √©galement fortement corr√©l√©e au rendement.\n",
        "\n",
        "- Le Seedrate et Trash montrent aussi un impact direct.\n",
        "\n",
        "> **Conclusion :** Ces variables sont essentielles pour la pr√©diction du rendement de paddy."
      ],
      "metadata": {
        "id": "XzB6qkEqcI4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Variables les plus importantes √† retenir pour mod√©lisation :\n",
        "\n",
        "- Nutriments : Micronutrients_70Days, Potassh_50Days, Urea_40Days, DAP_20days\n",
        "\n",
        "- Surface / parcelles : Hectares, Nursery area, LP_nurseryarea, LP_Mainfield\n",
        "\n",
        "- Pesticides et d√©sherbants : Pest_60Day, Weed28D_thiobencarb\n",
        "\n",
        "- Semences et trash : Seedrate, Trash\n",
        "\n",
        "2. Variables m√©t√©orologiques : bien que disponibles, leur corr√©lation lin√©aire est faible ‚Üí pourrait n√©cessiter transformation, agr√©gation ou mod√©lisation non-lin√©aire pour d√©tecter un effet.\n",
        "\n",
        "3. Implication pour le mod√®le ML :\n",
        "\n",
        "- Priorit√© aux variables fortement corr√©l√©es pour l‚Äôentra√Ænement initial.\n",
        "\n",
        "- Les variables faiblement corr√©l√©es ne sont pas forc√©ment inutiles : consid√©rer feature engineering (ex. indices cumul√©s, interactions, transformations non lin√©aires).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ODfL2bbTcGwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.9 Pairplot - Relations entre variables"
      ],
      "metadata": {
        "id": "pg77kEAJdTFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Pairplot des Variables les Plus Importantes\n",
        "# ========================================\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Variables les plus corr√©l√©es avec Paddy yield\n",
        "important_vars = [\n",
        "    'Micronutrients_70Days',\n",
        "    'Hectares ',\n",
        "    'Pest_60Day(in ml)',\n",
        "    'Nursery area (Cents)',\n",
        "    'Potassh_50Days',\n",
        "    'LP_nurseryarea(in Tonnes)',\n",
        "    'Weed28D_thiobencarb',\n",
        "    'Urea_40Days',\n",
        "    'LP_Mainfield(in Tonnes)',\n",
        "    'DAP_20days',\n",
        "    'Seedrate(in Kg)',\n",
        "    'Trash(in bundles)',\n",
        "    'Paddy yield(in Kg)'  # La variable cible pour colorer\n",
        "]\n",
        "\n",
        "# Pairplot pour visualiser les relations entre les variables importantes\n",
        "sns.pairplot(df[important_vars], hue='Paddy yield(in Kg)',\n",
        "             palette='viridis', plot_kws={'alpha':0.6}, diag_kind='kde')\n",
        "\n",
        "plt.suptitle('Pairplot des Variables les Plus Corr√©l√©es au Rendement de Paddy', y=1.02, fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OG3gCoaadVsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering & Seclection**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Tr1KaynHf3UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Le SelectKBest est une technique de feature selection automatique qui permet de\n",
        "s√©lectionner les K variables les plus importantes par rapport √† la variable cible. Voici une explication claire :\n",
        "\n",
        "1Ô∏è‚É£ Principe\n",
        "\n",
        "Pour chaque feature, SelectKBest calcule un score statistique mesurant la relation avec la variable cible.\n",
        "\n",
        "Ensuite, il garde les K features avec le score le plus √©lev√© et ignore les autres.\n",
        "\n",
        "Le score d√©pend de la nature de la cible :\n",
        "\n",
        "- Cible num√©rique (r√©gression) ‚Üí f_regression, mutual_info_regression\n",
        "\n",
        "- Cible cat√©gorielle (classification) ‚Üí chi2, f_classif, mutual_info_classif\n",
        "\n",
        "2Ô∏è‚É£ Pourquoi c‚Äôest utile\n",
        "\n",
        "- R√©duction de dimension : √©limine les variables peu pertinentes ‚Üí mod√®le plus simple et rapide.\n",
        "\n",
        "- R√©duction du bruit : les variables inutiles peuvent nuire √† la performance.\n",
        "\n",
        "- Am√©lioration de l‚Äôinterpr√©tabilit√© : on se concentre sur les features les plus importantes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rkmDURm1fOqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Target\n",
        "target = 'Paddy yield(in Kg)'\n",
        "y = df[target]\n",
        "X = df.drop(target, axis=1)\n",
        "\n",
        "print(f\"Nombre total de features avant s√©lection : {X.shape[1]}\")\n",
        "\n",
        "# ========================================\n",
        "# 1. Feature Selection avec SelectKBest (univari√©)\n",
        "# ========================================\n",
        "# On garde les 20 meilleures features selon f_regression\n",
        "selector = SelectKBest(score_func=f_regression, k=12)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# R√©cup√©rer les noms des features s√©lectionn√©es\n",
        "selected_features = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "print(f\"Features s√©lectionn√©es ({len(selected_features)}) :\")\n",
        "print(selected_features)\n",
        "\n",
        "# Nouveau DataFrame avec seulement les features s√©lectionn√©es + cible\n",
        "df_selected = df[selected_features + [target]]\n",
        "\n",
        "# ========================================\n",
        "# 2. Train-Test Split\n",
        "# ========================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_selected.drop(target, axis=1),\n",
        "    df_selected[target],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "# 3. Scaling\n",
        "# ========================================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain set : {X_train.shape}\")\n",
        "print(f\"Test set  : {X_test.shape}\")\n",
        "print(\"Donn√©es pr√™tes pour la mod√©lisation !\")"
      ],
      "metadata": {
        "id": "bz_3_3RGgxfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.\tTrain-Test Split & Scaling**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vjIp0TTWlyqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MOD√âLISATION**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ya5N2vTLmxBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "print(\"MOD√âLISATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Dictionnaire pour stocker les r√©sultats\n",
        "results = {}\n",
        "\n",
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Entra√Æne et √©value un mod√®le\n",
        "    \"\"\"\n",
        "    # Entra√Ænement\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Pr√©dictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # M√©triques sur l'ensemble d'entra√Ænement\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "    # M√©triques sur l'ensemble de test\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    # Validation crois√©e\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5,\n",
        "                                scoring='neg_root_mean_squared_error')\n",
        "    cv_rmse = -cv_scores.mean()\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Train_RMSE': train_rmse,\n",
        "        'Train_MAE': train_mae,\n",
        "        'Train_R2': train_r2,\n",
        "        'Test_RMSE': test_rmse,\n",
        "        'Test_MAE': test_mae,\n",
        "        'Test_R2': test_r2,\n",
        "        'CV_RMSE': cv_rmse,\n",
        "        'Predictions_Train': y_train_pred,\n",
        "        'Predictions_Test': y_test_pred\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AbwLdKuAl2i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 1. LINEAR REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n1. LINEAR REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "results['Linear Regression'] = evaluate_model('Linear Regression', lr_model,\n",
        "                                              X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Linear Regression']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Linear Regression']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Linear Regression']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Linear Regression']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['Linear Regression']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['Linear Regression']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Linear Regression']['CV_RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "IkODVCgOnC9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 2. LASSO REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n2. LASSO REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "lasso_model = Lasso(alpha=0.001, max_iter=10000, random_state=42)\n",
        "results['Lasso'] = evaluate_model('Lasso', lasso_model,\n",
        "                                 X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Lasso']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Lasso']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Lasso']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Lasso']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['Lasso']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['Lasso']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Lasso']['CV_RMSE']:.4f}\")\n",
        "\n",
        "# Nombre de coefficients non-nuls\n",
        "non_zero_coef = np.sum(lasso_model.coef_ != 0)\n",
        "print(f\"  Coefficients non-nuls: {non_zero_coef}/{len(lasso_model.coef_)}\")"
      ],
      "metadata": {
        "id": "FJJIN5QNnEdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 3. RIDGE REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n3. RIDGE REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "ridge_model = Ridge(alpha=10, random_state=42)\n",
        "results['Ridge'] = evaluate_model('Ridge', ridge_model,\n",
        "                                 X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Ridge']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Ridge']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Ridge']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Ridge']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['Ridge']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['Ridge']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Ridge']['CV_RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "jd9rb_kknGrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 4. ELASTIC NET\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n4. ELASTIC NET\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "elasticnet_model = ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000,\n",
        "                             random_state=42)\n",
        "results['ElasticNet'] = evaluate_model('ElasticNet', elasticnet_model,\n",
        "                                      X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['ElasticNet']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['ElasticNet']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['ElasticNet']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['ElasticNet']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['ElasticNet']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['ElasticNet']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['ElasticNet']['CV_RMSE']:.4f}\")\n",
        "\n",
        "non_zero_coef_en = np.sum(elasticnet_model.coef_ != 0)\n",
        "print(f\"  Coefficients non-nuls: {non_zero_coef_en}/{len(elasticnet_model.coef_)}\")"
      ],
      "metadata": {
        "id": "sYUlTdbvnJB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 5. XGBOOST REGRESSOR\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"\\n5. XGBOOST REGRESSOR\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Initialisation du mod√®le\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=500,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entra√Ænement et √©valuation\n",
        "results['XGBoost'] = evaluate_model('XGBoost', xgb_model,\n",
        "                                    X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "# Affichage des r√©sultats\n",
        "print(f\"  Train RMSE: {results['XGBoost']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['XGBoost']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['XGBoost']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['XGBoost']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['XGBoost']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['XGBoost']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['XGBoost']['CV_RMSE']:.4f}\")\n"
      ],
      "metadata": {
        "id": "r6dnAUSIB3qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# COMPARAISON DES MOD√àLES\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"COMPARAISON DES MOD√àLES\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Mod√®le': list(results.keys()),\n",
        "    'Train_RMSE': [results[m]['Train_RMSE'] for m in results.keys()],\n",
        "    'Test_RMSE': [results[m]['Test_RMSE'] for m in results.keys()],\n",
        "    'Train_MAE': [results[m]['Train_MAE'] for m in results.keys()],\n",
        "    'Test_MAE': [results[m]['Test_MAE'] for m in results.keys()],\n",
        "    'Train_R2': [results[m]['Train_R2'] for m in results.keys()],\n",
        "    'Test_R2': [results[m]['Test_R2'] for m in results.keys()],\n",
        "    'CV_RMSE': [results[m]['CV_RMSE'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "print(\"\\nTABLEAU COMPARATIF:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualisation de la comparaison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "models = list(results.keys())\n",
        "x_pos = np.arange(len(models))\n",
        "\n",
        "# RMSE Comparison\n",
        "axes[0, 0].bar(x_pos - 0.2, comparison_df['Train_RMSE'], 0.4,\n",
        "              label='Train RMSE', color='skyblue')\n",
        "axes[0, 0].bar(x_pos + 0.2, comparison_df['Test_RMSE'], 0.4,\n",
        "              label='Test RMSE', color='lightcoral')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0, 0].set_ylabel('RMSE')\n",
        "axes[0, 0].set_title('Comparaison RMSE (plus bas = meilleur)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# MAE Comparison\n",
        "axes[0, 1].bar(x_pos - 0.2, comparison_df['Train_MAE'], 0.4,\n",
        "              label='Train MAE', color='lightgreen')\n",
        "axes[0, 1].bar(x_pos + 0.2, comparison_df['Test_MAE'], 0.4,\n",
        "              label='Test MAE', color='gold')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].set_title('Comparaison MAE (plus bas = meilleur)')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# R¬≤ Comparison\n",
        "axes[1, 0].bar(x_pos - 0.2, comparison_df['Train_R2'], 0.4,\n",
        "              label='Train R¬≤', color='mediumpurple')\n",
        "axes[1, 0].bar(x_pos + 0.2, comparison_df['Test_R2'], 0.4,\n",
        "              label='Test R¬≤', color='orange')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylabel('R¬≤')\n",
        "axes[1, 0].set_title('Comparaison R¬≤ (plus haut = meilleur)')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "axes[1, 0].set_ylim([min(comparison_df['Test_R2'])-0.05, 1.0])\n",
        "\n",
        "# Overfitting analysis (diff√©rence Train-Test)\n",
        "overfit_rmse = comparison_df['Train_RMSE'] - comparison_df['Test_RMSE']\n",
        "axes[1, 1].bar(x_pos, overfit_rmse, color='salmon')\n",
        "axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1, 1].set_ylabel('Train RMSE - Test RMSE')\n",
        "axes[1, 1].set_title('Analyse Overfitting\\n(valeurs n√©gatives = bon)')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Pr√©dictions vs Valeurs r√©elles\n",
        "# Pr√©dictions vs Valeurs r√©elles\n",
        "n_models = len(models)\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    n_models, 1, figsize=(8, 4 * n_models)\n",
        ")\n",
        "\n",
        "# Cas o√π il n‚Äôy a qu‚Äôun seul mod√®le\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, model_name in enumerate(models):\n",
        "    y_pred = results[model_name]['Predictions_Test']\n",
        "\n",
        "    axes[i].scatter(y_test, y_pred, alpha=0.5)\n",
        "    axes[i].plot(\n",
        "        [y_test.min(), y_test.max()],\n",
        "        [y_test.min(), y_test.max()],\n",
        "        'r--', lw=2, label='Parfait'\n",
        "    )\n",
        "    axes[i].set_xlabel('Valeurs r√©elles')\n",
        "    axes[i].set_ylabel('Pr√©dictions')\n",
        "    axes[i].set_title(\n",
        "        f'{model_name} | R¬≤ test = {results[model_name][\"Test_R2\"]:.4f}'\n",
        "    )\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lMVhBic1nLwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ces r√©sultats sont bons et coh√©rents. Voici pourquoi :\n",
        "\n",
        "1. R¬≤ √©lev√© :\n",
        "\n",
        "- Train R¬≤ ‚âà 0.906\n",
        "\n",
        "- Test R¬≤ ‚âà 0.886\n",
        "‚Üí Le mod√®le explique ~89‚ÄØ% de la variance sur le jeu de test, ce qui est tr√®s correct pour des donn√©es agricoles o√π il y a souvent du bruit.\n",
        "\n",
        "2. RMSE et MAE raisonnables :\n",
        "\n",
        "- RMSE Train ‚âà 2735, Test ‚âà 2889\n",
        "\n",
        "- MAE Train ‚âà 1588, Test ‚âà 1688\n",
        "‚Üí L‚Äô√©cart Train/Test est faible, donc pas d‚Äôoverfitting notable.\n",
        "\n",
        "3. Validation crois√©e (CV_RMSE) coh√©rente avec Test RMSE ‚Üí le mod√®le est stable.\n",
        "\n",
        "4. Comparaison des mod√®les lin√©aires :\n",
        "\n",
        "- Linear Regression, Lasso, Ridge, ElasticNet donnent presque les m√™mes performances ‚Üí aucune r√©gularisation forte n‚Äôest n√©cessaire, ce qui sugg√®re que les features s√©lectionn√©es sont d√©j√† pertinentes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SvpZWakHnQx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# III. S√âLECTION DE VARIABLES - BACKWARD ELIMINATION\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"PARTIE III: S√âLECTION DE VARIABLES PAR BACKWARD ELIMINATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"BACKWARD ELIMINATION\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "def backward_elimination(X, y, significance_level=0.05):\n",
        "    \"\"\"\n",
        "    Effectue une √©limination backward bas√©e sur les p-values\n",
        "\n",
        "    Param√®tres:\n",
        "    -----------\n",
        "    X : DataFrame\n",
        "        Features\n",
        "    y : Series\n",
        "        Target\n",
        "    significance_level : float\n",
        "        Seuil de significativit√© (d√©faut: 0.05)\n",
        "\n",
        "    Retourne:\n",
        "    ---------\n",
        "    selected_features : list\n",
        "        Liste des features s√©lectionn√©es\n",
        "    \"\"\"\n",
        "    features = list(X.columns)\n",
        "    removed_features = []\n",
        "\n",
        "    print(f\"\\nD√©but: {len(features)} variables\")\n",
        "    print(f\"Seuil de significativit√©: {significance_level}\")\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "\n",
        "        # Ajouter une constante\n",
        "        X_with_const = sm.add_constant(X[features])\n",
        "\n",
        "        # Ajuster le mod√®le\n",
        "        model = sm.OLS(y, X_with_const).fit()\n",
        "\n",
        "        # Trouver la variable avec la p-value la plus √©lev√©e\n",
        "        p_values = model.pvalues.iloc[1:]  # Exclure la constante\n",
        "        max_p_value = p_values.max()\n",
        "\n",
        "        if max_p_value > significance_level:\n",
        "            excluded_feature = p_values.idxmax()\n",
        "            features.remove(excluded_feature)\n",
        "            removed_features.append((excluded_feature, max_p_value))\n",
        "\n",
        "            if iteration <= 10 or iteration % 10 == 0:\n",
        "                print(f\"It√©ration {iteration}: Retrait de '{excluded_feature}' (p-value={max_p_value:.4f})\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"\\nTermin√© apr√®s {iteration} it√©rations\")\n",
        "    print(f\"Variables conserv√©es: {len(features)}\")\n",
        "    print(f\"Variables retir√©es: {len(removed_features)}\")\n",
        "\n",
        "    return features, removed_features\n",
        "\n",
        "# Appliquer backward elimination\n",
        "selected_features, removed_features = backward_elimination(X_train, y_train,\n",
        "                                                          significance_level=0.05)\n",
        "\n",
        "print(f\"\\n{len(selected_features)} variables s√©lectionn√©es sur {X_train.shape[1]}\")\n",
        "print(f\"R√©duction: {(1 - len(selected_features)/X_train.shape[1])*100:.1f}%\")\n",
        "\n",
        "# Afficher quelques variables retir√©es\n",
        "print(\"\\nPremi√®res variables retir√©es (Top 10 p-values):\")\n",
        "removed_sorted = sorted(removed_features, key=lambda x: x[1], reverse=True)\n",
        "for feat, pval in removed_sorted[:10]:\n",
        "    print(f\"  ‚Ä¢ {feat}: p-value = {pval:.6f}\")\n",
        "\n",
        "# Pr√©parer les nouveaux ensembles train/test avec les variables s√©lectionn√©es\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "print(f\"\\nNouvelles dimensions:\")\n",
        "print(f\"  X_train: {X_train_selected.shape}\")"
      ],
      "metadata": {
        "id": "62jnxp_R5Z2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# BACKWARD ELIMINATION\n",
        "# ===========================\n",
        "selected_features, removed_features = backward_elimination(X_train, y_train,\n",
        "                                                          significance_level=0.05)\n",
        "\n",
        "print(f\"\\n{len(selected_features)} variables s√©lectionn√©es sur {X_train.shape[1]}\")\n",
        "print(f\"R√©duction: {(1 - len(selected_features)/X_train.shape[1])*100:.1f}%\")\n",
        "\n",
        "# Afficher les 10 premi√®res variables retir√©es\n",
        "removed_sorted = sorted(removed_features, key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nPremi√®res variables retir√©es (Top 10 p-values):\")\n",
        "for feat, pval in removed_sorted[:10]:\n",
        "    print(f\"  ‚Ä¢ {feat}: p-value = {pval:.6f}\")\n",
        "\n",
        "# ===========================\n",
        "# Pr√©parer les ensembles train/test s√©lectionn√©s\n",
        "# ===========================\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "print(f\"\\nNouvelles dimensions apr√®s s√©lection:\")\n",
        "print(f\"  X_train: {X_train_selected.shape}\")\n",
        "print(f\"  X_test : {X_test_selected.shape}\")\n",
        "\n",
        "# ===========================\n",
        "# Appliquer le scaling seulement sur les features s√©lectionn√©es\n",
        "# ===========================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "print(\"\\nDonn√©es pr√™tes pour la mod√©lisation avec features s√©lectionn√©es !\")\n"
      ],
      "metadata": {
        "id": "sWqsc8Ke4Juk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 1. LINEAR REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n1. LINEAR REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "results['Linear Regression'] = evaluate_model('Linear Regression', lr_model,\n",
        "                                              X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Linear Regression']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Linear Regression']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Linear Regression']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Linear Regression']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['Linear Regression']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['Linear Regression']['Test_R2']:.4f}\")"
      ],
      "metadata": {
        "id": "JYTsnf_S5FXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 2. LASSO REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n2. LASSO REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "lasso_model = Lasso(alpha=0.001, max_iter=10000, random_state=42)\n",
        "results['Lasso'] = evaluate_model('Lasso', lasso_model,\n",
        "                                 X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Lasso']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Lasso']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Lasso']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Lasso']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['Lasso']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['Lasso']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Lasso']['CV_RMSE']:.4f}\")\n",
        "\n",
        "# Nombre de coefficients non-nuls\n",
        "non_zero_coef = np.sum(lasso_model.coef_ != 0)\n",
        "print(f\"  Coefficients non-nuls: {non_zero_coef}/{len(lasso_model.coef_)}\")"
      ],
      "metadata": {
        "id": "ezreEquM5MyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 3. RIDGE REGRESSION\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n3. RIDGE REGRESSION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "ridge_model = Ridge(alpha=10, random_state=42)\n",
        "results['Ridge'] = evaluate_model('Ridge', ridge_model,\n",
        "                                 X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['Ridge']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['Ridge']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['Ridge']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['Ridge']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['Ridge']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['Ridge']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['Ridge']['CV_RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "S_e750nX5jID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 4. ELASTIC NET\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n4. ELASTIC NET\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "elasticnet_model = ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000,\n",
        "                             random_state=42)\n",
        "results['ElasticNet'] = evaluate_model('ElasticNet', elasticnet_model,\n",
        "                                      X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(f\"  Train RMSE: {results['ElasticNet']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['ElasticNet']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['ElasticNet']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['ElasticNet']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['ElasticNet']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['ElasticNet']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['ElasticNet']['CV_RMSE']:.4f}\")\n",
        "\n",
        "non_zero_coef_en = np.sum(elasticnet_model.coef_ != 0)\n",
        "print(f\"  Coefficients non-nuls: {non_zero_coef_en}/{len(elasticnet_model.coef_)}\")"
      ],
      "metadata": {
        "id": "SIl1vOS95pd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 5. XGBOOST REGRESSOR\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"\\n5. XGBOOST REGRESSOR\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Initialisation du mod√®le\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=500,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entra√Ænement et √©valuation\n",
        "results['XGBoost'] = evaluate_model('XGBoost', xgb_model,\n",
        "                                    X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "# Affichage des r√©sultats\n",
        "print(f\"  Train RMSE: {results['XGBoost']['Train_RMSE']:.4f}\")\n",
        "print(f\"  Test RMSE:  {results['XGBoost']['Test_RMSE']:.4f}\")\n",
        "print(f\"  Train MAE:  {results['XGBoost']['Train_MAE']:.4f}\")\n",
        "print(f\"  Test MAE:   {results['XGBoost']['Test_MAE']:.4f}\")\n",
        "print(f\"  Train R¬≤:   {results['XGBoost']['Train_R2']:.4f}\")\n",
        "print(f\"  Test R¬≤:    {results['XGBoost']['Test_R2']:.4f}\")\n",
        "print(f\"  CV RMSE:    {results['XGBoost']['CV_RMSE']:.4f}\")\n"
      ],
      "metadata": {
        "id": "Nw8OMCIKChZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# COMPARAISON DES MOD√àLES\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"COMPARAISON DES MOD√àLES\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Mod√®le': list(results.keys()),\n",
        "    'Train_RMSE': [results[m]['Train_RMSE'] for m in results.keys()],\n",
        "    'Test_RMSE': [results[m]['Test_RMSE'] for m in results.keys()],\n",
        "    'Train_MAE': [results[m]['Train_MAE'] for m in results.keys()],\n",
        "    'Test_MAE': [results[m]['Test_MAE'] for m in results.keys()],\n",
        "    'Train_R2': [results[m]['Train_R2'] for m in results.keys()],\n",
        "    'Test_R2': [results[m]['Test_R2'] for m in results.keys()],\n",
        "    'CV_RMSE': [results[m]['CV_RMSE'] for m in results.keys()],\n",
        "})\n",
        "\n",
        "print(\"\\nTABLEAU COMPARATIF:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualisation de la comparaison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "models = list(results.keys())\n",
        "x_pos = np.arange(len(models))\n",
        "\n",
        "# RMSE Comparison\n",
        "axes[0, 0].bar(x_pos - 0.2, comparison_df['Train_RMSE'], 0.4,\n",
        "              label='Train RMSE', color='skyblue')\n",
        "axes[0, 0].bar(x_pos + 0.2, comparison_df['Test_RMSE'], 0.4,\n",
        "              label='Test RMSE', color='lightcoral')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0, 0].set_ylabel('RMSE')\n",
        "axes[0, 0].set_title('Comparaison RMSE (plus bas = meilleur)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# MAE Comparison\n",
        "axes[0, 1].bar(x_pos - 0.2, comparison_df['Train_MAE'], 0.4,\n",
        "              label='Train MAE', color='lightgreen')\n",
        "axes[0, 1].bar(x_pos + 0.2, comparison_df['Test_MAE'], 0.4,\n",
        "              label='Test MAE', color='gold')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].set_title('Comparaison MAE (plus bas = meilleur)')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# R¬≤ Comparison\n",
        "axes[1, 0].bar(x_pos - 0.2, comparison_df['Train_R2'], 0.4,\n",
        "              label='Train R¬≤', color='mediumpurple')\n",
        "axes[1, 0].bar(x_pos + 0.2, comparison_df['Test_R2'], 0.4,\n",
        "              label='Test R¬≤', color='orange')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylabel('R¬≤')\n",
        "axes[1, 0].set_title('Comparaison R¬≤ (plus haut = meilleur)')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "axes[1, 0].set_ylim([min(comparison_df['Test_R2'])-0.05, 1.0])\n",
        "\n",
        "# Overfitting analysis (diff√©rence Train-Test)\n",
        "overfit_rmse = comparison_df['Train_RMSE'] - comparison_df['Test_RMSE']\n",
        "axes[1, 1].bar(x_pos, overfit_rmse, color='salmon')\n",
        "axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[1, 1].set_ylabel('Train RMSE - Test RMSE')\n",
        "axes[1, 1].set_title('Analyse Overfitting\\n(valeurs n√©gatives = bon)')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Pr√©dictions vs Valeurs r√©elles\n",
        "n_models = len(models)\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    n_models, 1, figsize=(8, 4 * n_models)\n",
        ")\n",
        "\n",
        "# Cas o√π il n‚Äôy a qu‚Äôun seul mod√®le\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, model_name in enumerate(models):\n",
        "    y_pred = results[model_name]['Predictions_Test']\n",
        "\n",
        "    axes[i].scatter(y_test, y_pred, alpha=0.5)\n",
        "    axes[i].plot(\n",
        "        [y_test.min(), y_test.max()],\n",
        "        [y_test.min(), y_test.max()],\n",
        "        'r--', lw=2, label='Parfait'\n",
        "    )\n",
        "    axes[i].set_xlabel('Valeurs r√©elles')\n",
        "    axes[i].set_ylabel('Pr√©dictions')\n",
        "    axes[i].set_title(\n",
        "        f'{model_name} | R¬≤ test = {results[model_name][\"Test_R2\"]:.4f}'\n",
        "    )\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-wuImtfO5wri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Analyse de ton tableau\n",
        "\n",
        "Linear Regression & Lasso & ElasticNet :\n",
        "\n",
        "- Test_RMSE ‚âà 2888‚Äì2888.6\n",
        "\n",
        "- Test_R¬≤ ‚âà 0.8862\n",
        "\n",
        "- Train/Test tr√®s proches ‚Üí pas de sur-apprentissage\n",
        "\n",
        "Ridge :\n",
        "\n",
        "- Test_RMSE l√©g√®rement plus bas (2887.12)\n",
        "\n",
        "- Test_R¬≤ l√©g√®rement plus haut (0.8863)\n",
        "\n",
        "Diff√©rence minimale mais l√©g√®rement meilleure performance g√©n√©rale\n",
        "\n",
        "> Conclusion :\n",
        "\n",
        "- Ridge Regression a un tout petit avantage sur le Test_RMSE et Test_R¬≤ ‚Üí il est l√©g√®rement le meilleur mod√®le sur ces donn√©es.\n",
        "\n",
        "- Les autres mod√®les sont pratiquement √©quivalents, donc Linear, Lasso ou ElasticNet restent tr√®s performants aussi."
      ],
      "metadata": {
        "id": "EFUFQ7Qg6SKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV**"
      ],
      "metadata": {
        "id": "1rghTkiN88ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# ------------------------------------------------------------\n",
        "# 1. D√©finir les mod√®les optimis√©s (apr√®s GridSearchCV)\n",
        "# ------------------------------------------------------------\n",
        "optimized_models = {\n",
        "    \"Ridge Optimis√©\": Ridge(alpha=10, random_state=42),\n",
        "    \"Lasso Optimis√©\": Lasso(alpha=0.0001, max_iter=10000, random_state=42),\n",
        "    \"ElasticNet Optimis√©\": ElasticNet(alpha=0.1, l1_ratio=0.9, max_iter=10000, random_state=42)\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Fonction pour entra√Æner et √©valuer un mod√®le\n",
        "# ------------------------------------------------------------\n",
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Metrics\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    # CV RMSE\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "    cv_rmse = -cv_scores.mean()\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Train_RMSE': train_rmse,\n",
        "        'Test_RMSE': test_rmse,\n",
        "        'Train_MAE': train_mae,\n",
        "        'Test_MAE': test_mae,\n",
        "        'Train_R2': train_r2,\n",
        "        'Test_R2': test_r2,\n",
        "        'CV_RMSE': cv_rmse\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. √âvaluer tous les mod√®les optimis√©s\n",
        "# ------------------------------------------------------------\n",
        "results_optimized = {}\n",
        "for name, model in optimized_models.items():\n",
        "    results_optimized[name] = evaluate_model(name, model, X_train_selected, X_test_selected, y_train, y_test)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Cr√©er le tableau comparatif final\n",
        "# ------------------------------------------------------------\n",
        "comparison_df_optimized = pd.DataFrame({\n",
        "    'Mod√®le': [results_optimized[m]['Model'] for m in results_optimized],\n",
        "    'Train_RMSE': [results_optimized[m]['Train_RMSE'] for m in results_optimized],\n",
        "    'Test_RMSE': [results_optimized[m]['Test_RMSE'] for m in results_optimized],\n",
        "    'Train_MAE': [results_optimized[m]['Train_MAE'] for m in results_optimized],\n",
        "    'Test_MAE': [results_optimized[m]['Test_MAE'] for m in results_optimized],\n",
        "    'Train_R2': [results_optimized[m]['Train_R2'] for m in results_optimized],\n",
        "    'Test_R2': [results_optimized[m]['Test_R2'] for m in results_optimized],\n",
        "    'CV_RMSE': [results_optimized[m]['CV_RMSE'] for m in results_optimized],\n",
        "})\n",
        "\n",
        "print(\"\\nTABLEAU COMPARATIF DES MOD√àLES OPTIMIS√âS:\")\n",
        "print(comparison_df_optimized.to_string(index=False))"
      ],
      "metadata": {
        "id": "YZnr8nJL6Rx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Analyse\n",
        "\n",
        "- Test_RMSE (erreur sur test) : Lasso Optimis√© est l√©g√®rement meilleur (2665.97 < 2666.08 et < 2666.24).\n",
        "\n",
        "- Test_R¬≤ (qualit√© du mod√®le sur test) : Lasso Optimis√© a le R¬≤ le plus √©lev√© (0.90619).\n",
        "\n",
        "- Train/Test gap : tr√®s petit pour tous ‚Üí pas de surapprentissage significatif.\n",
        "\n",
        "- CV_RMSE (validation crois√©e) : Ridge et ElasticNet ont un CV_RMSE un peu plus faible, mais les diff√©rences sont tr√®s minimes.\n",
        "\n",
        "> Conclusion\n",
        "\n",
        "- Tous les trois mod√®les sont tr√®s performants et proches les uns des autres.\n",
        "\n",
        "- Lasso Optimis√© pourrait √™tre consid√©r√© comme le meilleur l√©ger avantage sur le Test_RMSE et R¬≤."
      ],
      "metadata": {
        "id": "A74h-QSy-cCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Probl√®me de Classification : Pr√©diction de la Vari√©t√© de Paddy (Riz)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-B7Wp0PyFWB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìã D√©finition du Probl√®me**\n",
        "\n",
        "**1. Contexte Agricole**\n",
        "\n",
        "Dans la culture du paddy (riz) en Inde (districts comme Cuddalore, Kurinjipadi, etc.), le choix de la vari√©t√© de semence (ex. : CO_43, ponmani, delux ponni) est critique pour maximiser le rendement en fonction des conditions locales :\n",
        "\n",
        "- Facteurs influents : Type de sol (alluvial, clay), conditions m√©t√©orologiques (pluie par p√©riode : 30DRain, temp√©ratures min/max par phase de croissance, vent, humidit√©), pratiques culturales (Nursery: dry/wet, intrants comme DAP_20days, Urea_40Days, pesticides), bloc agricole (Agriblock), superficie (Hectares), et rendement observ√© (Paddy yield(in Kg)).\n",
        "- D√©fi r√©el : Les agriculteurs doivent s√©lectionner la vari√©t√© optimale avant plantation, bas√©e sur des donn√©es historiques/environnementales, pour optimiser rendement et r√©sistance (ex. : ponmani tol√®re mieux l'argile humide, CO_43 alluvial sec).\n",
        "\n",
        "**2. Probl√®me ML**\n",
        "\n",
        "Classification multi-classe pour pr√©dire la Vari√©t√© de Paddy ('CO_43', 'ponmani', 'delux ponni') √† partir des features agronomiques et m√©t√©o.\n",
        "\n",
        "- Objectif : Recommander la vari√©t√© la plus adapt√©e (r√©duire risques, ‚Üë rendement de 10-20% en moyenne).\n",
        "- Impact : Outil d√©cisionnel pour fermiers (ex. : app mobile input sol/pluie ‚Üí output vari√©t√©)."
      ],
      "metadata": {
        "id": "j3zzw-umF9MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1. EDA (Exploratory Data Analysis)\n",
        "---"
      ],
      "metadata": {
        "id": "rTzmSjirNh4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du dataset\n",
        "df = pd.read_csv(\"cleaned_paddydataset.csv\")\n",
        "print(\"Shape du dataset:\", df.shape)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IKGvaseoFf3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Informations sur le dataset:\")\n",
        "print(\"=\"*80)\n",
        "# Informations g√©n√©rales sur le dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "mYtBfaMGQm4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Statistiques descriptives:\")\n",
        "print(\"=\"*80)\n",
        "# Statistiques descriptives\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "CGoJzpieN3gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Division des variables num√©riques et cat√©goriques"
      ],
      "metadata": {
        "id": "hsbROMcuSVR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©paration des variables num√©riques et cat√©goriques\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_cols = [col for col in categorical_cols if col != 'Variety']\n",
        "\n",
        "# Print\n",
        "print(\"Variables num√©riques:\")\n",
        "print(numerical_cols)\n",
        "print(f\"\\nNombre de variables num√©riques: {len(numerical_cols)}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nVariables cat√©goriques:\")\n",
        "print(categorical_cols)\n",
        "print(f\"\\nNombre de variables cat√©goriques: {len(categorical_cols)}\")"
      ],
      "metadata": {
        "id": "dX2yoDp0SWOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 V√©rification des valeurs manquantes"
      ],
      "metadata": {
        "id": "lGOZeep3TBjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# V√©rification des valeurs manquantes\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Valeurs_Manquantes': missing_values,\n",
        "    'Pourcentage': missing_percentage\n",
        "})\n",
        "\n",
        "missing_df = missing_df[missing_df['Valeurs_Manquantes'] > 0].sort_values(\n",
        "    'Valeurs_Manquantes', ascending=False\n",
        ")\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"Valeurs manquantes d√©tect√©es:\")\n",
        "    print(missing_df)\n",
        "else:\n",
        "    print(\"‚úì Aucune valeur manquante d√©tect√©e dans le dataset!\")"
      ],
      "metadata": {
        "id": "VhsQEyehTC-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Analyse de la variable cible (Variety)"
      ],
      "metadata": {
        "id": "DablF0FEQwzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution de la variable cible : Variety\n",
        "target_counts = df['Variety'].value_counts()\n",
        "target_percentage = df['Variety'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Distribution de la variable cible 'Variety':\")\n",
        "print(\"=\" * 60)\n",
        "for variety in target_counts.index:\n",
        "    print(f\"{variety:15} : {target_counts[variety]:4d} √©chantillons ({target_percentage[variety]:.2f}%)\")\n",
        "\n",
        "# Tri des vari√©t√©s par ordre alphab√©tique ou par count pour une meilleure lisibilit√©\n",
        "target_counts = target_counts.sort_values(ascending=False)\n",
        "\n",
        "# Visualisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# 1. Bar plot\n",
        "bars = axes[0].bar(target_counts.index, target_counts.values,\n",
        "                   color=['#3498db', '#e67e22', '#27ae60'],\n",
        "                   alpha=0.85, edgecolor='black', linewidth=1.2)\n",
        "axes[0].set_xlabel('Vari√©t√© de Paddy', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Nombre d\\'√©chantillons', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Distribution de la variable cible (Variety)', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Ajouter les valeurs au-dessus des barres\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                 f'{int(height)}',\n",
        "                 ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# 2. Pie chart\n",
        "colors = ['#3498db', '#e67e22', '#27ae60']\n",
        "wedges, texts, autotexts = axes[1].pie(target_counts.values,\n",
        "                                      labels=target_counts.index,\n",
        "                                      autopct='%1.1f%%',\n",
        "                                      colors=colors,\n",
        "                                      startangle=90,\n",
        "                                      wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})\n",
        "\n",
        "# Am√©liorer la lisibilit√© des pourcentages\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontweight('bold')\n",
        "    autotext.set_fontsize(11)\n",
        "\n",
        "axes[1].set_title('R√©partition des Vari√©t√©s de Paddy', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7pYrhKHBQsX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Analyse des variables cat√©goriques"
      ],
      "metadata": {
        "id": "6vcOnImLTkIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse du Variety par variable cat√©gorique\n",
        "# Nombre de colonnes par ligne (2)\n",
        "cols_per_row = 2\n",
        "n_cols = categorical_cols\n",
        "n_plots = len(n_cols)\n",
        "\n",
        "# Calculer la grille\n",
        "n_rows = int(np.ceil(n_plots / cols_per_row))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(20, 5 * n_rows))\n",
        "axes = axes.flatten() if n_rows > 1 else axes  # Flatten pour it√©ration facile\n",
        "\n",
        "# Couleurs pour les 3 vari√©t√©s\n",
        "colors = ['#3498db', '#e67e22', '#27ae60']  # Bleu, Orange, Vert\n",
        "\n",
        "for idx, col in enumerate(n_cols):\n",
        "    # Crosstab en pourcentage par ligne (distribution de Variety pour chaque modalit√© de col)\n",
        "    ct = pd.crosstab(df[col], df['Variety'], normalize='index') * 100\n",
        "\n",
        "    # Trier les colonnes pour ordre fixe (CO_43, ponmani, delux ponni)\n",
        "    variety_order = sorted(df['Variety'].unique())  # ou ['co_43', 'ponmani', 'delux ponni']\n",
        "    ct = ct[variety_order]\n",
        "\n",
        "    # Bar plot (non stacked pour comparer facilement)\n",
        "    ct.plot(kind='bar', ax=axes[idx], color=colors, alpha=0.85, edgecolor='black')\n",
        "\n",
        "    axes[idx].set_title(f'Distribution de Variety par {col}', fontsize=13, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Pourcentage (%)', fontsize=12)\n",
        "    axes[idx].set_xlabel(col, fontsize=12)\n",
        "    axes[idx].legend(title='Variety', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Masquer les subplots vides si n_plots n'est pas multiple de cols_per_row\n",
        "for idx in range(n_plots, len(axes)):\n",
        "    axes[idx].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2AoxKDT6TgN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Distribution des variables num√©riques"
      ],
      "metadata": {
        "id": "67QQCPplUlyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogrammes des variables num√©riques\n",
        "# Param√®tres de la grille\n",
        "cols_per_row = 3          # 3 histogrammes par ligne\n",
        "n_plots = len(numerical_cols)\n",
        "n_cols = cols_per_row\n",
        "n_rows = int(np.ceil(n_plots / n_cols))\n",
        "\n",
        "# Cr√©ation de la figure\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
        "axes = axes.flatten() if n_rows > 1 else axes  # Pour it√©rer facilement\n",
        "\n",
        "# Couleur principale\n",
        "hist_color = '#3498db'  # Bleu agr√©able (tu peux changer)\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    # Histogramme\n",
        "    axes[idx].hist(df[col].dropna(), bins=30, color=hist_color,\n",
        "                   alpha=0.75, edgecolor='black', linewidth=1.2)\n",
        "\n",
        "    axes[idx].set_title(f'Distribution de {col}', fontsize=13, fontweight='bold')\n",
        "    axes[idx].set_xlabel(col, fontsize=11)\n",
        "    axes[idx].set_ylabel('Fr√©quence', fontsize=11)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Statistiques descriptives : moyenne et m√©diane\n",
        "    mean_val = df[col].mean()\n",
        "    median_val = df[col].median()\n",
        "\n",
        "    axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2,\n",
        "                      label=f'Moyenne: {mean_val:.2f}')\n",
        "    axes[idx].axvline(median_val, color='green', linestyle='--', linewidth=2,\n",
        "                      label=f'M√©diane: {median_val:.2f}')\n",
        "\n",
        "    axes[idx].legend(fontsize=9)\n",
        "\n",
        "# Masquer les subplots inutilis√©s (si nombre de variables non multiple de cols_per_row)\n",
        "for idx in range(n_plots, len(axes)):\n",
        "    axes[idx].set_visible(False)\n",
        "\n",
        "plt.suptitle('Distributions des Variables Num√©riques', fontsize=16, fontweight='bold', y=0.98)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Ajustement pour le titre global\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "95GmYwLvUneT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Box plots - D√©tection des outliers"
      ],
      "metadata": {
        "id": "vEm0KlUuVYep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plots pour d√©tecter les outliers (TOUTES les variables num√©riques, par Variety)\n",
        "\n",
        "print(f\"Nombre total de variables num√©riques : {len(numerical_cols)}\")\n",
        "\n",
        "# Param√®tres de la grille\n",
        "cols_per_row = 2         # On garde 2 colonnes par ligne comme dans ton code original\n",
        "n_plots = len(numerical_cols)\n",
        "n_rows = int(np.ceil(n_plots / cols_per_row))\n",
        "\n",
        "# Cr√©ation de la figure avec le nombre de lignes n√©cessaire\n",
        "fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(20, 5 * n_rows))\n",
        "if n_rows > 1:\n",
        "    axes = axes.ravel()\n",
        "else:\n",
        "    axes = axes  # Pour √©viter l'erreur quand n_rows == 1\n",
        "\n",
        "# Couleurs pour les 3 vari√©t√©s (proche de ton style original)\n",
        "colors = ['#2ecc71', '#e67e22', '#3498db']  # Vert, Orange, Bleu\n",
        "\n",
        "# Liste ordonn√©e des vari√©t√©s pour coh√©rence\n",
        "variety_labels = ['co_43', 'ponmani', 'delux ponni']\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    # Pr√©paration des donn√©es pour les 3 vari√©t√©s\n",
        "    data_to_plot = [\n",
        "        df[df['Variety'] == variety][col].dropna() for variety in variety_labels\n",
        "    ]\n",
        "\n",
        "    bp = axes[idx].boxplot(data_to_plot,\n",
        "                           labels=variety_labels,\n",
        "                           patch_artist=True,\n",
        "                           widths=0.6)\n",
        "\n",
        "    # Colorier les box plots (3 bo√Ætes)\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.6)\n",
        "\n",
        "    axes[idx].set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel(col, fontsize=10)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Masquer les subplots vides (s'il y en a)\n",
        "for idx in range(n_plots, len(axes)):\n",
        "    axes[idx].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UWVO2cnLVb-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 3. Encodage des Variables Cat√©gorielles\n",
        "# ========================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"ENCODAGE DES VARIABLES CAT√âGORIELLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# V√©rification du nombre de cat√©gories uniques (avant encodage)\n",
        "print(\"Nombre de cat√©gories par variable :\")\n",
        "for col in categorical_features:\n",
        "    if col in df.columns:\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"   {col:30} ‚Üí {unique_count} cat√©gories uniques\")\n",
        "    else:\n",
        "        print(f\"   {col:30} ‚Üí COLONNE MANQUANTE !\")\n",
        "\n",
        "# Application de One-Hot Encoding\n",
        "# dtype=int : pour avoir 0/1 au lieu de bool\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=False, dtype=int)\n",
        "\n",
        "print(f\"\\nDimensions avant encodage : {df.shape}\")\n",
        "print(f\"Dimensions apr√®s encodage  : {df_encoded.shape}\")\n",
        "\n",
        "# Calcul du nombre de nouvelles colonnes cr√©√©es\n",
        "new_columns_created = df_encoded.shape[1] - (df.shape[1] - len(categorical_features))\n",
        "print(f\"Nouvelles colonnes binaires cr√©√©es : {new_columns_created}\")\n",
        "\n",
        "# Exemple des nouvelles colonnes g√©n√©r√©es\n",
        "print(\"\\nExemples de colonnes One-Hot cr√©√©es :\")\n",
        "new_cols = [col for col in df_encoded.columns if col not in df.columns]\n",
        "print(new_cols[:20])  # Affiche les 20 premi√®res\n",
        "\n",
        "# Mise √† jour du DataFrame pour la suite du notebook\n",
        "df = df_encoded.copy()\n",
        "\n",
        "# Aper√ßu final\n",
        "print(\"\\nAper√ßu du dataset encod√© :\")\n",
        "print(df.head())\n",
        "print(f\"\\nTypes de colonnes : {df.dtypes.value_counts()}\")"
      ],
      "metadata": {
        "id": "vYRPsRDRaf8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Matrice de corr√©lation"
      ],
      "metadata": {
        "id": "DEvRMhH9ZUqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identification automatique des colonnes One-Hot li√©es √† Variety\n",
        "variety_onehot_cols = [col for col in df.columns if col.startswith('Variety_')]\n",
        "\n",
        "print(\"Colonnes One-Hot pour Variety d√©tect√©es :\")\n",
        "print(variety_onehot_cols)\n",
        "# Colonnes √† inclure dans la matrice de corr√©lation\n",
        "correlation_cols = numerical_cols + variety_onehot_cols\n",
        "\n",
        "# Calcul de la matrice de corr√©lation\n",
        "corr_matrix = df[correlation_cols].corr()\n",
        "\n",
        "# Heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            linewidths=1,\n",
        "            cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Matrice de Corr√©lation des Variables Num√©riques\\n(avec colonnes One-Hot de Variety)',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Corr√©lation avec chaque vari√©t√© (via les colonnes One-Hot)\n",
        "print(\"\\nCorr√©lation des variables num√©riques avec chaque vari√©t√© :\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# On affiche les corr√©lations pour chaque colonne One-Hot, tri√©es par valeur absolue\n",
        "for variety_col in variety_onehot_cols:\n",
        "    print(f\"\\n--- Corr√©lation avec {variety_col.replace('Variety_', '')} ---\")\n",
        "    target_corr = corr_matrix[variety_col].drop(variety_onehot_cols)  # On enl√®ve les corr√©lations entre One-Hot\n",
        "    target_corr = target_corr.sort_values(ascending=False, key=abs)\n",
        "    print(target_corr.head(10))  # Top 10 des plus fortes associations\n",
        "    print(\"...\")\n",
        "\n",
        "# Option bonus : corr√©lation moyenne absolue (force globale d'association avec la vari√©t√©)\n",
        "print(\"\\n--- Force moyenne d'association (valeur absolue) avec la vari√©t√© ---\")\n",
        "mean_abs_corr = corr_matrix[variety_onehot_cols].drop(variety_onehot_cols).abs().mean(axis=1).sort_values(ascending=False)\n",
        "print(mean_abs_corr.head(15))"
      ],
      "metadata": {
        "id": "hNnSjBDEZYL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 S√©paration des features et de la target"
      ],
      "metadata": {
        "id": "-mKT6bChePZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©paration X et y\n",
        "# S√©paration X et y (apr√®s One-Hot Encoding avec drop_first=True)\n",
        "\n",
        "# Identification automatique des colonnes One-Hot cr√©√©es pour Variety\n",
        "variety_onehot_cols = [col for col in df.columns if col.startswith('Variety_')]\n",
        "\n",
        "print(\"Colonnes One-Hot d√©tect√©es pour la cible 'Variety' :\")\n",
        "print(variety_onehot_cols)\n",
        "\n",
        "# S√©paration features / target\n",
        "X = df.drop(variety_onehot_cols, axis=1)   # Toutes les colonnes sauf les One-Hot de Variety\n",
        "y = df[variety_onehot_cols]                # y = les colonnes One-Hot (format multi-colonnes)\n",
        "\n",
        "print(f\"Shape de X: {X.shape}\")\n",
        "print(f\"Shape de y: {y.shape}\")\n",
        "print(f\"\\nNombre de features: {X.shape[1]}\")\n",
        "print(f\"\\nListe des features:\")\n",
        "print(X.columns.tolist()[:30])  # Affiche les 30 premi√®res"
      ],
      "metadata": {
        "id": "sG-wAfhPeytT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Split Train/Test"
      ],
      "metadata": {
        "id": "woDjXT0AiJXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour la mod√©lisation\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "NrFf9KeaiKnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split des donn√©es\n",
        "\n",
        "# Conversion one-hot (n,3) ‚Üí labels (n,)\n",
        "y_labels = np.argmax(y.values, axis=1)\n",
        "\n",
        "# Split stratifi√©\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_labels\n",
        ")\n",
        "\n",
        "print(\"Split des donn√©es (80% train - 20% test):\")\n",
        "print(\"=\"*80)\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape : {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape : {y_test.shape}\")\n",
        "\n",
        "# Noms des vari√©t√©s\n",
        "variety_names = [col.replace('Variety_', '') for col in y.columns]\n",
        "\n",
        "print(\"\\nDistribution de la variable cible (Variety):\")\n",
        "print(\"-\"*80)\n",
        "for i, variety in enumerate(variety_names):\n",
        "    train_count = np.sum(y_train == i)\n",
        "    test_count  = np.sum(y_test == i)\n",
        "\n",
        "    print(f\"Train - {variety:12} : {train_count:4d} ({train_count/len(y_train)*100:5.2f}%)\")\n",
        "    print(f\"Test  - {variety:12} : {test_count:4d} ({test_count/len(y_test)*100:5.2f}%)\")"
      ],
      "metadata": {
        "id": "XIsJ5KUFiPmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Normalisation des donn√©es"
      ],
      "metadata": {
        "id": "wTkGzdFkk55c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisation des features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Normalisation effectu√©e avec StandardScaler\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nStatistiques apr√®s normalisation (X_train):\")\n",
        "print(f\"Moyenne: {X_train_scaled.mean(axis=0).mean():.6f}\")\n",
        "print(f\"√âcart-type: {X_train_scaled.std(axis=0).mean():.6f}\")\n",
        "\n",
        "# Conversion en DataFrame pour plus de clart√©\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)"
      ],
      "metadata": {
        "id": "TCmuuVd7k6uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 3. Mod√©lisation & √âvaluation\n",
        "---"
      ],
      "metadata": {
        "id": "IwAW_PzzlBAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fonction g√©n√©rique d'√©valuation (binaire & multi-classes)\n",
        "    \"\"\"\n",
        "\n",
        "    # Pr√©dictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    n_classes = len(np.unique(y_test))\n",
        "\n",
        "    # ROC-AUC\n",
        "    roc_auc = None\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "        if n_classes == 2:\n",
        "            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "        else:\n",
        "            roc_auc = roc_auc_score(\n",
        "                y_test,\n",
        "                y_pred_proba,\n",
        "                multi_class='ovr',\n",
        "                average='weighted'\n",
        "            )\n",
        "\n",
        "    # M√©triques\n",
        "    results = {\n",
        "        'Mod√®le': name,\n",
        "        'Accuracy Train': accuracy_score(y_train, y_pred_train),\n",
        "        'Accuracy Test': accuracy_score(y_test, y_pred_test),\n",
        "        'Precision': precision_score(y_test, y_pred_test, average='weighted'),\n",
        "        'Recall': recall_score(y_test, y_pred_test, average='weighted'),\n",
        "        'F1-Score': f1_score(y_test, y_pred_test, average='weighted'),\n",
        "        'ROC-AUC': roc_auc\n",
        "    }\n",
        "\n",
        "    # Affichage des r√©sultats\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MOD√àLE: {name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for metric, value in results.items():\n",
        "        if metric != 'Mod√®le':\n",
        "            if value is not None:\n",
        "                print(f\"{metric:20s}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"{metric:20s}: N/A\")\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(y_test, y_pred_test)\n",
        "    class_labels = np.unique(y_test)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_labels,\n",
        "                yticklabels=class_labels)\n",
        "    plt.title(f'Matrice de Confusion - {name}', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Vraie Classe')\n",
        "    plt.xlabel('Classe Pr√©dite')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Rapport de classification\n",
        "    print(\"\\nRapport de classification:\")\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "3dsSWqFik96t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "PoEYao02lLEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour l'√©valuation\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "# Pour la s√©lection de features\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif"
      ],
      "metadata": {
        "id": "z1XFYc8GlLw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Entra√Ænement du mod√®le KNN\n",
        "print(\"Entra√Ænement du mod√®le K-Nearest Neighbors...\")\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# √âvaluation\n",
        "results_knn = evaluate_model('K-Nearest Neighbors', knn,\n",
        "                             X_train_scaled, X_test_scaled,\n",
        "                             y_train, y_test)"
      ],
      "metadata": {
        "id": "lMQG6bMIlN9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Logistic Regression"
      ],
      "metadata": {
        "id": "JfSYeeXyFlwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement du mod√®le Logistic Regression\n",
        "print(\"Entra√Ænement du mod√®le Logistic Regression...\")\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# √âvaluation\n",
        "results_lr = evaluate_model('Logistic Regression', lr,\n",
        "                            X_train_scaled, X_test_scaled,\n",
        "                            y_train, y_test)"
      ],
      "metadata": {
        "id": "fFsXyUuMFmks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Decision Tree avec visualisation"
      ],
      "metadata": {
        "id": "fVRol1vCFqia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement du mod√®le Decision Tree\n",
        "print(\"Entra√Ænement du mod√®le Decision Tree...\")\n",
        "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "dt.fit(X_train_scaled, y_train)\n",
        "\n",
        "# √âvaluation\n",
        "results_dt = evaluate_model('Decision Tree', dt,\n",
        "                           X_train_scaled, X_test_scaled,\n",
        "                           y_train, y_test)"
      ],
      "metadata": {
        "id": "Eof4vVFQFtxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualisation de l'arbre de d√©cision\n",
        "plt.figure(figsize=(22, 12))\n",
        "\n",
        "plot_tree(\n",
        "    dt,                                  # le mod√®le entra√Æn√©\n",
        "    feature_names=X.columns,             # noms des features\n",
        "    class_names=dt.classes_.astype(str), # classes r√©elles\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=9\n",
        ")\n",
        "\n",
        "plt.title(\"Visualisation de l'Arbre de D√©cision\", fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F4JQIUxaF0T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Random Forest avec Feature Importance"
      ],
      "metadata": {
        "id": "89aUS7z3F5zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement du mod√®le Random Forest\n",
        "print(\"Entra√Ænement du mod√®le Random Forest...\")\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# √âvaluation\n",
        "results_rf = evaluate_model('Random Forest', rf,\n",
        "                           X_train_scaled, X_test_scaled,\n",
        "                           y_train, y_test)"
      ],
      "metadata": {
        "id": "J3-ETRenF3QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=True)\n",
        "\n",
        "# Visualisation avec barres horizontales\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(feature_importance)))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'],\n",
        "         color=colors, edgecolor='black', alpha=0.8)\n",
        "plt.xlabel('Importance', fontsize=2, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=5, fontweight='bold')\n",
        "plt.title('Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', alpha=0.8)\n",
        "\n",
        "# Ajouter les valeurs\n",
        "for i, v in enumerate(feature_importance['Importance']):\n",
        "    plt.text(v + 0.001, i, f'{v:.4f}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 5 Features les plus importantes:\")\n",
        "print(feature_importance.tail(5).to_string(index=False))"
      ],
      "metadata": {
        "id": "JmE81VrDF_0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 XGBoost"
      ],
      "metadata": {
        "id": "xRbmSbf9GIcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement du mod√®le XGBoost\n",
        "print(\"Entra√Ænement du mod√®le XGBoost...\")\n",
        "xgb = XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# √âvaluation\n",
        "results_xgb = evaluate_model('XGBoost', xgb,\n",
        "                            X_train_scaled, X_test_scaled,\n",
        "                            y_train, y_test)"
      ],
      "metadata": {
        "id": "ELY3x0xEGJJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 Comparaison des mod√®les baseline"
      ],
      "metadata": {
        "id": "2FdfeR3QGOjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison des r√©sultats\n",
        "comparison_df = pd.DataFrame([results_knn, results_lr, results_dt, results_rf, results_xgb])\n",
        "comparison_df = comparison_df.set_index('Mod√®le')\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPARAISON DES MOD√àLES BASELINE\")\n",
        "print(\"=\"*100)\n",
        "print(comparison_df.to_string())\n",
        "\n",
        "# Visualisation comparative\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "metrics = ['Accuracy Test', 'Precision', 'Recall', 'F1-Score']\n",
        "colors_palette = sns.color_palette('husl', len(comparison_df))\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "\n",
        "    comparison_df[metric].plot(kind='bar', ax=axes[row, col],\n",
        "                               color=colors_palette, alpha=0.8, edgecolor='black')\n",
        "    axes[row, col].set_title(f'Comparaison - {metric}', fontsize=13, fontweight='bold')\n",
        "    axes[row, col].set_ylabel('Score', fontsize=11)\n",
        "    axes[row, col].set_xlabel('')\n",
        "    axes[row, col].grid(axis='y', alpha=0.3)\n",
        "    axes[row, col].set_xticklabels(axes[row, col].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(comparison_df[metric]):\n",
        "        axes[row, col].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XwYAKY48GPcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "print(\"\\n=== MOD√àLES SANS PCA ===\")\n",
        "\n",
        "# ==============================\n",
        "# Decision Trees\n",
        "# ==============================\n",
        "dt_gini = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_entropy = DecisionTreeClassifier(\n",
        "    criterion='entropy',\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_gini.fit(X_train_scaled, y_train)\n",
        "dt_entropy.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_dt_gini = dt_gini.predict(X_test_scaled)\n",
        "y_pred_dt_entropy = dt_entropy.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nDecision Tree - Gini\")\n",
        "print(classification_report(y_test, y_pred_dt_gini))\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred_dt_gini))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred_dt_gini, average='weighted'))\n",
        "\n",
        "print(\"\\nDecision Tree - Entropy\")\n",
        "print(classification_report(y_test, y_pred_dt_entropy))\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred_dt_entropy))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred_dt_entropy, average='weighted'))\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Random Forest\n",
        "# ==============================\n",
        "rf_gini = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    criterion='gini',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_entropy = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    criterion='entropy',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_gini.fit(X_train_scaled, y_train)\n",
        "rf_entropy.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_rf_gini = rf_gini.predict(X_test_scaled)\n",
        "y_pred_rf_entropy = rf_entropy.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nRandom Forest - Gini\")\n",
        "print(classification_report(y_test, y_pred_rf_gini))\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred_rf_gini))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred_rf_gini, average='weighted'))\n",
        "\n",
        "print(\"\\nRandom Forest - Entropy\")\n",
        "print(classification_report(y_test, y_pred_rf_entropy))\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred_rf_entropy))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred_rf_entropy, average='weighted'))\n"
      ],
      "metadata": {
        "id": "OjIhsvXGPTSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le but d‚Äôutiliser Gini ou Entropie dans les arbres de d√©cision et les for√™ts al√©atoires est le m√™me :\n",
        "\n",
        "`Trouver la meilleure s√©paration possible √† chaque n≈ìud pour que les sous-n≈ìuds soient les plus purs possibles.`\n",
        "\n",
        "Autrement dit :\n",
        "\n",
        "- On veut que chaque branche contienne des exemples aussi homog√®nes que possible (ex. tous de la m√™me classe).\n",
        "\n",
        "- Gini et Entropie sont juste deux fa√ßons diff√©rentes de mesurer l‚Äôimpuret√© ou le d√©sordre d‚Äôun n≈ìud.\n",
        "\n",
        "- L‚Äôarbre utilise cette mesure pour d√©cider quelle variable et quelle valeur de split choisir √† chaque √©tape."
      ],
      "metadata": {
        "id": "01dvXAaUPySc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 4. Fine-tuning des Meilleurs Mod√®les\n",
        "---"
      ],
      "metadata": {
        "id": "9RmnvbebGYzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 S√©lection des meilleurs mod√®les"
      ],
      "metadata": {
        "id": "Cy5dvaDuGb0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©lection des 3 meilleurs mod√®les bas√©s sur le F1-Score\n",
        "best_models_idx = comparison_df['F1-Score'].nlargest(3).index\n",
        "print(\"Top 3 mod√®les s√©lectionn√©s pour le fine-tuning:\")\n",
        "print(\"=\"*80)\n",
        "for i, model_name in enumerate(best_models_idx, 1):\n",
        "    print(f\"{i}. {model_name} - F1-Score: {comparison_df.loc[model_name, 'F1-Score']:.4f}\")"
      ],
      "metadata": {
        "id": "y3KCb5CfGZw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Fine-tuning avec GridSearchCV"
      ],
      "metadata": {
        "id": "aXaIkuBTGgfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionnaire des mod√®les et param√®tres √† tuner\n",
        "models_to_tune = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'params': {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l2'],\n",
        "            'solver': ['lbfgs', 'liblinear']\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [5, 10, 15, 20, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [3, 5, 7, 9],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'subsample': [0.8, 0.9, 1.0],\n",
        "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Stockage des mod√®les tun√©s\n",
        "tuned_models = {}\n",
        "tuned_results = []\n",
        "\n",
        "for model_name, config in models_to_tune.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FINE-TUNING: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # GridSearchCV\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=config['model'],\n",
        "        param_grid=config['params'],\n",
        "        cv=5,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    print(f\"\\nMeilleurs param√®tres trouv√©s:\")\n",
        "    for param, value in grid_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "    print(f\"\\nMeilleur score (F1) en validation crois√©e: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Stocker le meilleur mod√®le\n",
        "    tuned_models[model_name] = grid_search.best_estimator_\n",
        "\n",
        "    # √âvaluation sur le test set\n",
        "    results = evaluate_model(f'{model_name} (Tuned)',\n",
        "                            grid_search.best_estimator_,\n",
        "                            X_train_scaled, X_test_scaled,\n",
        "                            y_train, y_test)\n",
        "    tuned_results.append(results)"
      ],
      "metadata": {
        "id": "qL7IVUjOGhJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Comparaison Avant/Apr√®s Fine-tuning"
      ],
      "metadata": {
        "id": "r9chc-ejGr1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison des r√©sultats\n",
        "tuned_comparison_df = pd.DataFrame(tuned_results).set_index('Mod√®le')\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPARAISON DES MOD√àLES APR√àS FINE-TUNING\")\n",
        "print(\"=\"*100)\n",
        "print(tuned_comparison_df.to_string())\n",
        "\n",
        "# Visualisation comparative\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "x = np.arange(len(tuned_comparison_df))\n",
        "width = 0.15\n",
        "\n",
        "metrics = ['Accuracy Test', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
        "    ax.bar(x + i*width, tuned_comparison_df[metric], width,\n",
        "           label=metric, color=color, alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax.set_xlabel('Mod√®les', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Comparaison des M√©triques - Mod√®les Tun√©s', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(tuned_comparison_df.index, rotation=15, ha='right')\n",
        "ax.legend(loc='lower right', fontsize=10)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VW1dVjsZGs36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Courbes ROC des mod√®les tun√©s"
      ],
      "metadata": {
        "id": "U_FL_UY-GvGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
        "\n",
        "# Pour chaque mod√®le tun√©\n",
        "for model_name, model in tuned_models.items():\n",
        "    # Pr√©dictions probabilistes pour toutes les classes\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)  # shape (n_samples, n_classes)\n",
        "\n",
        "    # ROC-AUC multi-classe (One-vs-Rest)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "\n",
        "    print(f\"{model_name:25s}: ROC-AUC = {roc_auc:.4f}\")\n",
        "\n",
        "# Ligne diagonale (classifieur al√©atoire)\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Classifieur Al√©atoire (AUC = 0.5)')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=12, fontweight='bold')\n",
        "plt.title('Courbes ROC - Mod√®les Tun√©s', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nScores ROC-AUC des mod√®les tun√©s:\")\n",
        "print(\"=\"*80)\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "\n",
        "for (model_name, model) in tuned_models.items():\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)\n",
        "    plt.figure(figsize=(8,5))\n",
        "\n",
        "    for i, color in zip(range(y_pred_proba.shape[1]), colors):\n",
        "        fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba[:, i])\n",
        "        plt.plot(fpr, tpr, color=color, lw=2.5, label=f'Classe {i}')\n",
        "\n",
        "    plt.plot([0,1], [0,1], 'k--', lw=2)\n",
        "    plt.xlabel(\"FPR\")\n",
        "    plt.ylabel(\"TPR\")\n",
        "    plt.title(f\"ROC multi-classe - {model_name}\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bawopIgbGy8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Comparaison des mod√®les (ROC-AUC multi-classe)\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "fOMal7bjWSjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse des courbes ROC multi-classe :\n",
        "\n",
        "- XGBoost : Courbes tr√®s hautes et proches du coin sup√©rieur gauche ‚Üí tr√®s bonne s√©paration des 3 classes.\n",
        "- Random Forest : Bonnes courbes, un peu moins nettes que XGBoost.\n",
        "- Logistic Regression : Courbes basses, surtout pour la classe rouge ‚Üí difficult√© √† s√©parer les classes (mod√®le trop simple)."
      ],
      "metadata": {
        "id": "pR1qyYSWWw-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 5. S√©lection de Caract√©ristiques avec SelectKBest\n",
        "---"
      ],
      "metadata": {
        "id": "qeLpy_nGG2hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 S√©lection des K meilleures features"
      ],
      "metadata": {
        "id": "EqIklGyKHBLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SelectKBest avec diff√©rentes valeurs de K\n",
        "k_values = [5, 7, 10]\n",
        "feature_selection_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"S√âLECTION DES {k} MEILLEURES FEATURES\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # SelectKBest\n",
        "    selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "    X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "    # Obtenir les noms des features s√©lectionn√©es\n",
        "    selected_features = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "    print(f\"\\nFeatures s√©lectionn√©es ({k}):\")\n",
        "    for i, feature in enumerate(selected_features, 1):\n",
        "        print(f\"  {i}. {feature}\")\n",
        "\n",
        "    # Scores des features\n",
        "    feature_scores = pd.DataFrame({\n",
        "        'Feature': X.columns[selector.get_support()],\n",
        "        'Score': selector.scores_[selector.get_support()]\n",
        "    }).sort_values('Score', ascending=False)\n",
        "\n",
        "    print(f\"\\nScores des features:\")\n",
        "    print(feature_scores.to_string(index=False))\n",
        "\n",
        "    # Tester avec diff√©rents mod√®les\n",
        "    models_test = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
        "    }\n",
        "\n",
        "    for model_name, model in models_test.items():\n",
        "        model.fit(X_train_selected, y_train)\n",
        "        y_pred = model.predict(X_test_selected)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        feature_selection_results.append({\n",
        "            'K': k,\n",
        "            'Mod√®le': model_name,\n",
        "            'Accuracy': accuracy,\n",
        "            'F1-Score': f1\n",
        "        })\n",
        "\n",
        "        print(f\"\\n{model_name} avec {k} features:\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "v_M5Ih6eHAVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Visualisation des scores avec SelectKBest"
      ],
      "metadata": {
        "id": "vjdYl4niHHgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualiser tous les scores des features\n",
        "selector_all = SelectKBest(score_func=f_classif, k='all')\n",
        "selector_all.fit(X_train_scaled, y_train)\n",
        "\n",
        "all_feature_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Score': selector_all.scores_\n",
        "}).sort_values('Score', ascending=True)\n",
        "\n",
        "# Visualisation avec barres horizontales\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = plt.cm.plasma(np.linspace(0, 1, len(all_feature_scores)))\n",
        "plt.barh(all_feature_scores['Feature'], all_feature_scores['Score'],\n",
        "         color=colors, edgecolor='black', alpha=0.8)\n",
        "plt.xlabel('Score F-value', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
        "plt.title('Scores des Features - SelectKBest (f_classif)', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Ajouter les valeurs\n",
        "for i, v in enumerate(all_feature_scores['Score']):\n",
        "    plt.text(v + 5, i, f'{v:.1f}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hhbmeb8XHKPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Comparaison des performances avec diff√©rents K"
      ],
      "metadata": {
        "id": "chFUJ9T7HOap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison des r√©sultats\n",
        "fs_results_df = pd.DataFrame(feature_selection_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"R√âSULTATS DE LA S√âLECTION DE FEATURES\")\n",
        "print(\"=\"*100)\n",
        "print(fs_results_df.to_string(index=False))\n",
        "\n",
        "# Visualisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Accuracy\n",
        "for model_name in fs_results_df['Mod√®le'].unique():\n",
        "    model_data = fs_results_df[fs_results_df['Mod√®le'] == model_name]\n",
        "    axes[0].plot(model_data['K'], model_data['Accuracy'],\n",
        "                marker='o', linewidth=2.5, markersize=10, label=model_name)\n",
        "\n",
        "axes[0].set_xlabel('Nombre de Features (K)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Accuracy vs Nombre de Features', fontsize=13, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].set_xticks(k_values)\n",
        "\n",
        "# F1-Score\n",
        "for model_name in fs_results_df['Mod√®le'].unique():\n",
        "    model_data = fs_results_df[fs_results_df['Mod√®le'] == model_name]\n",
        "    axes[1].plot(model_data['K'], model_data['F1-Score'],\n",
        "                marker='o', linewidth=2.5, markersize=10, label=model_name)\n",
        "\n",
        "axes[1].set_xlabel('Nombre de Features (K)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('F1-Score vs Nombre de Features', fontsize=13, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].set_xticks(k_values)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "luS4lx_jHSSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 6. Conclusion\n",
        "---"
      ],
      "metadata": {
        "id": "mrcLuPLsXZ0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\" \"*30 + \"RAPPORT FINAL - PR√âDICTION DES VARI√âT√âS DE PADDY\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "print(\"\\nüìä R√âSUM√â DE L'ANALYSE:\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "print(\"\\n1. EXPLORATION DES DONN√âES (EDA):\")\n",
        "print(f\"   ‚Ä¢ Dataset de {df.shape[0]} parcelles agricoles avec {df.shape[1]} variables initiales\")\n",
        "print(f\"   ‚Ä¢ 3 vari√©t√©s de riz : CO_43, ponmani, delux ponni (classes √©quilibr√©es ~33% chacune)\")\n",
        "print(f\"   ‚Ä¢ Variables num√©riques : {len(numerical_cols)} (pluies, temp√©ratures, humidit√©, intrants...)\")\n",
        "print(f\"   ‚Ä¢ Variables cat√©gorielles : {len(categorical_cols)} (Agriblock, Soil Types, Nursery, directions du vent)\")\n",
        "print(\"   ‚Ä¢ Aucune valeur manquante majeure apr√®s nettoyage\")\n",
        "print(\"   ‚Ä¢ Fortes corr√©lations observ√©es entre variables m√©t√©o (pluies par p√©riode, temp√©ratures min/max)\")\n",
        "\n",
        "print(\"\\n2. PREPROCESSING:\")\n",
        "print(\"   ‚Ä¢ Nettoyage : suppression espaces, gestion NaN, normalisation texte\")\n",
        "print(\"   ‚Ä¢ Encodage One-Hot des variables cat√©gorielles (drop_first=True pour √©viter multicolin√©arit√©)\")\n",
        "print(\"   ‚Ä¢ Standardisation avec StandardScaler (obligatoire pour PCA et mod√®les sensibles √† l'√©chelle)\")\n",
        "print(\"   ‚Ä¢ Application de PCA : r√©duction √† 95% de variance expliqu√©e\")\n",
        "\"\"\"\n",
        "print(f\"   ‚Ä¢ R√©duction dimensionnelle : {original_n_features} ‚Üí {pca.n_components_} composantes principales\")\n",
        "print(\"   ‚Ä¢ Split Train/Test : 80/20 avec stratification\")\n",
        "\n",
        "print(\"\\n3. MOD√âLISATION ET COMPARAISON CRIT√àRES DE SPLIT (Gini vs Entropy):\")\n",
        "print(\"-\" * 100)\n",
        "# Suppose que tu as un DataFrame comparison_df avec ces colonnes\n",
        "# √Ä adapter selon tes vrais r√©sultats\n",
        "print(comparison_df[['Mod√®le', 'Crit√®re', 'Avec PCA', 'Accuracy Test', 'F1-Score', 'ROC-AUC']].to_string(index=False))\n",
        "\n",
        "best_model = comparison_df.loc[comparison_df['ROC-AUC'].idxmax()]\n",
        "print(f\"\\n   ‚úì Meilleur mod√®le global : {best_model['Mod√®le']} avec {best_model['Crit√®re']} et {'PCA' if best_model['Avec PCA'] else 'sans PCA'}\")\n",
        "print(f\"   ‚úì ROC-AUC : {best_model['ROC-AUC']:.4f} | F1-Score : {best_model['F1-Score']:.4f}\")\n",
        "\n",
        "print(\"\\n4. IMPACT DE LA PCA ET DU CRIT√àRE DE SPLIT:\")\n",
        "print(\"-\" * 100)\n",
        "print(\"   ‚Ä¢ PCA : r√©duction massive des features tout en conservant 95% de la variance\")\n",
        "print(\"   ‚Ä¢ Meilleure performance souvent obtenue avec PCA (moins de bruit, meilleure g√©n√©ralisation)\")\n",
        "print(\"   ‚Ä¢ Entropy (Shannon) l√©g√®rement sup√©rieur √† Gini sur Random Forest et Decision Tree\")\n",
        "print(\"   ‚Ä¢ XGBoost domine largement (ROC-AUC = 0.8718) gr√¢ce √† sa gestion optimis√©e des interactions\")\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üéØ RECOMMANDATIONS FINALES:\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "print(\"\\n1. MOD√àLE RECOMMAND√â:\")\n",
        "print(\"   XGBoost avec PCA (95% variance) et crit√®re par d√©faut (similaire √† Entropy)\")\n",
        "print(\"   ‚Üí Meilleur pouvoir discriminant (ROC-AUC = 0.8718)\")\n",
        "print(\"   ‚Üí Robuste et performant sur donn√©es agricoles complexes\")\n",
        "\n",
        "print(\"\\n2. MOD√àLE ALTERNATIF INTERPR√âTABLE:\")\n",
        "print(\"   Random Forest avec crit√®re Entropy et PCA\")\n",
        "print(\"   ‚Üí Tr√®s bon compromis performance/interpr√©tabilit√© (ROC-AUC ‚âà 0.80)\")\n",
        "print(\"   ‚Üí Permet d'extraire l'importance des features (pluies, temp√©ratures, sol...)\")\n",
        "\n",
        "print(\"\\n3. FEATURES LES PLUS INFLUENTES (d'apr√®s Random Forest/XGBoost):\")\n",
        "print(\"   ‚Ä¢ Variables m√©t√©o : pluies par p√©riode (30DRain, 30_50DRain, etc.)\")\n",
        "print(\"   ‚Ä¢ Temp√©ratures min/max par phase de croissance\")\n",
        "print(\"   ‚Ä¢ Type de sol (alluvial vs clay)\")\n",
        "print(\"   ‚Ä¢ Bloc agricole (Agriblock) et m√©thode de p√©pini√®re (Nursery)\")\n",
        "print(\"   ‚Ä¢ Intrants (DAP, Urea, pesticides)\")\n",
        "\n",
        "print(\"\\n4. ACTIONS AGRONOMIQUES:\")\n",
        "print(\"   ‚Ä¢ Utiliser le mod√®le pour recommander la vari√©t√© optimale selon les conditions locales\")\n",
        "print(\"   ‚Ä¢ Identifier les parcelles √† risque de faible rendement avec une vari√©t√© mal adapt√©e\")\n",
        "print(\"   ‚Ä¢ Guider les agriculteurs vers la vari√©t√© la plus adapt√©e (ex. CO_43 en zone humide)\")\n",
        "print(\"   ‚Ä¢ Optimiser l'utilisation des intrants selon la vari√©t√© choisie\")\n",
        "\n",
        "print(\"\\n5. PROCHAINES √âTAPES:\")\n",
        "print(\"   ‚Ä¢ D√©ployer le mod√®le dans une application mobile/web pour les agriculteurs\")\n",
        "print(\"   ‚Ä¢ Ajouter des donn√©es m√©t√©o en temps r√©el pour des pr√©dictions saisonni√®res\")\n",
        "print(\"   ‚Ä¢ Tester le clustering non supervis√© pour d√©couvrir de nouveaux profils agro-climatiques\")\n",
        "print(\"   ‚Ä¢ Int√©grer des donn√©es satellites ou IoT pour enrichir le dataset\")\n",
        "print(\"   ‚Ä¢ R√©entra√Æner p√©riodiquement avec de nouvelles campagnes agricoles\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\" \"*25 + \"FIN DE L'ANALYSE - PROJET DATA SCIENCE AGRICOLE\")\n",
        "print(\"=\"*100 + \"\\n\")"
      ],
      "metadata": {
        "id": "bsGC9qJEXuWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}